{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for CLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import lmdb\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import vector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "These are defined in `data_processing/cld_processing` but are reproduced here clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "pion_mass = 0.13957\n",
    "B = -2.0  # magnetic field in T (-2 for CLD FCC-ee)\n",
    "c = 3e8  # speed of light in m/s\n",
    "scale = 1000\n",
    "\n",
    "# append path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path(\"/mnt/ceph/users/ewulff/particlemind\")))\n",
    "\n",
    "from data_processing.cld_processing import (\n",
    "    get_event_data,\n",
    "    gen_to_features,\n",
    "    track_to_features,\n",
    "    cluster_to_features,\n",
    "    process_calo_hit_data,\n",
    "    process_tracker_hit_data,\n",
    "    create_track_to_hit_coo_matrix,\n",
    "    create_cluster_to_hit_coo_matrix,\n",
    "    genparticle_track_adj,\n",
    "    create_genparticle_to_genparticle_coo_matrix,\n",
    "    create_genparticle_to_genparticle_coo_matrix2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to extract:\n",
    "- [x] hits (all tracker and calo hit features in the event)\n",
    "- [x] genparticles (all genparticle features in the event, noting that the genparticles are in the form of a decay tree)\n",
    "- [x] genparticle_to_genparticle (sparse association matrix for the genparticle decay tree)\n",
    "- [x] hits_to_genparticles (sparse association matrix)\n",
    "- [x] tracks (all track features in the event)\n",
    "- [x] tracks_to_hits (sparse association matrix between tracks and tracker hits)\n",
    "- [x] clusters (all cluster features in the event)\n",
    "- [x] clusters_to_hits (sparse association matrix between clusters and calo hits)\n",
    "\n",
    "Additional data to extract:\n",
    "- [x] genparticle_to_track (sparse association matrix for the genparticle to track associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files_dir = Path(\"/mnt/ceph/users/ewulff/data/cld/Dec3/subfolder_0/\")\n",
    "root_file = root_files_dir / \"reco_p8_ee_tt_ecm365_10000.root\"\n",
    "fi = uproot.open(root_file)\n",
    "ev = fi[\"events\"]\n",
    "\n",
    "# which event to pick from the file\n",
    "iev = 2\n",
    "\n",
    "collectionIDs = {\n",
    "    k: v\n",
    "    for k, v in zip(\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\"events___idTable/m_collectionIDs\"][0],\n",
    "    )\n",
    "}\n",
    "\n",
    "event_data = get_event_data(ev)\n",
    "\n",
    "# Extract gparticle, track, and cluster features\n",
    "gen_features = gen_to_features(event_data, iev)\n",
    "track_features = track_to_features(event_data, iev)\n",
    "cluster_features = cluster_to_features(\n",
    "    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    ")\n",
    "\n",
    "# Process calorimeter hit data\n",
    "calo_hit_features, genparticle_to_calo_hit_matrix, calo_hit_idx_local_to_global = process_calo_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Process tracker hit data\n",
    "tracker_hit_features, genparticle_to_tracker_hit_matrix, tracker_hit_idx_local_to_global = process_tracker_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Create the track-to-trackerhit adjacency matrix\n",
    "track_to_tracker_hit_matrix, tracker_hit_idx_local_to_global_2 = create_track_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    tracker_hit_idx_local_to_global == tracker_hit_idx_local_to_global_2\n",
    "), \"Local to global tracker hit index mapping mismatch!\"\n",
    "\n",
    "# Ceate the cluster-to-clusterhit adjacency matrix\n",
    "cluster_to_cluster_hit_matrix, calo_hit_idx_local_to_global_2 = create_cluster_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    calo_hit_idx_local_to_global == calo_hit_idx_local_to_global_2\n",
    "), \"Local to global calorimeter hit index mapping mismatch!\"\n",
    "\n",
    "# Create the genparticle-to-track adjacency matrix\n",
    "gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "\n",
    "# Create the genparticle-to-genparticle adjacency matrix\n",
    "gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "# Check consistency between the two methods gp-to-gp methods\n",
    "gp_to_gp2 = create_genparticle_to_genparticle_coo_matrix2(event_data, iev)\n",
    "\n",
    "# Get the number of genparticles in the event\n",
    "n_gp = len(ev[\"MCParticles.momentum.x\"].array()[iev])\n",
    "\n",
    "# create coo matrix through the COOs in gp_to_gp and gp_to_gp2\n",
    "coo_matrix_gp_to_gp = coo_matrix(\n",
    "    (gp_to_gp[\"weight\"], (gp_to_gp[\"parent_idx\"], gp_to_gp[\"daughter_idx\"])), shape=(n_gp, n_gp)\n",
    ")\n",
    "coo_matrix_gp_to_gp2 = coo_matrix(\n",
    "    (gp_to_gp2[\"weight\"], (gp_to_gp2[\"parent_idx\"], gp_to_gp2[\"daughter_idx\"])), shape=(n_gp, n_gp)\n",
    ")\n",
    "# Check if the two dense gp_to_gp matrices are equal\n",
    "assert (coo_matrix_gp_to_gp.todense() == coo_matrix_gp_to_gp2.todense()).all()\n",
    "# Define the output file path\n",
    "output_file = Path(\"extracted_features.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features), type(track_features), type(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features[\"px\"]), type(track_features[\"px\"]), type(cluster_features[\"position.x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features), type(tracker_hit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features[\"energy\"]), type(tracker_hit_features[\"energy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix),\n",
    "    type(genparticle_to_tracker_hit_matrix),\n",
    "    type(track_to_tracker_hit_matrix),\n",
    "    type(cluster_to_cluster_hit_matrix),\n",
    "    type(gp_to_gp),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix[\"hit_idx\"]),\n",
    "    type(genparticle_to_tracker_hit_matrix[\"gen_idx\"]),\n",
    "    type(track_to_tracker_hit_matrix[\"track_idx\"]),\n",
    "    type(cluster_to_cluster_hit_matrix[\"cluster_idx\"]),\n",
    "    type(gp_to_gp[\"parent_idx\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from calo_hit_features and tracker_hit_features into a single dataframe\n",
    "df = pandas.DataFrame()\n",
    "\n",
    "# Extract data from calo_hit_features and tracker_hit_features using numpy.concatenate\n",
    "df[\"px\"] = np.concatenate([calo_hit_features[\"position.x\"], tracker_hit_features[\"position.x\"]])\n",
    "df[\"py\"] = np.concatenate([calo_hit_features[\"position.y\"], tracker_hit_features[\"position.y\"]])\n",
    "df[\"pz\"] = np.concatenate([calo_hit_features[\"position.z\"], tracker_hit_features[\"position.z\"]])\n",
    "df[\"energy\"] = np.concatenate([1000 * calo_hit_features[\"energy\"], 1000 * tracker_hit_features[\"energy\"]])\n",
    "df[\"plotsize\"] = 0.0\n",
    "df[\"subdetector\"] = np.concatenate([calo_hit_features[\"subdetector\"], tracker_hit_features[\"subdetector\"]])\n",
    "\n",
    "# Calculate plotsize based on subdetector values\n",
    "df.loc[df[\"subdetector\"] == 0, \"plotsize\"] = df.loc[df[\"subdetector\"] == 0, \"energy\"] / 5.0\n",
    "df.loc[df[\"subdetector\"] == 1, \"plotsize\"] = df.loc[df[\"subdetector\"] == 1, \"energy\"] / 10.0\n",
    "df.loc[df[\"subdetector\"] == 2, \"plotsize\"] = df.loc[df[\"subdetector\"] == 2, \"energy\"] * 100.0\n",
    "df.loc[df[\"subdetector\"] == 3, \"plotsize\"] = df.loc[df[\"subdetector\"] == 3, \"energy\"] * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks\n",
    "def helix_eq(charge, bfield, v):\n",
    "    \"\"\"Calculate the 3D helical trajectory of a charged particle in a magnetic field.\n",
    "\n",
    "    This function computes the x, y, and z coordinates of a charged particle's\n",
    "    helical trajectory in a uniform magnetic field over a range of time values.\n",
    "\n",
    "    Args:\n",
    "        charge (float): The charge of the particle in elementary charge units.\n",
    "        bfield (float): The magnetic field strength in Tesla.\n",
    "        v (vector): A vector object representing the particle's velocity and properties.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of three lists (x, y, z) representing the particle's\n",
    "        trajectory in 3D space:\n",
    "            - x (list): x-coordinates of the trajectory.\n",
    "            - y (list): y-coordinates of the trajectory.\n",
    "            - z (list): z-coordinates of the trajectory.\n",
    "    \"\"\"\n",
    "    R = v.pt / (charge * 0.3 * bfield)\n",
    "    omega = charge * 0.3 * bfield / (v.gamma * v.mass)\n",
    "    t_values = np.linspace(0, 2 / (c * v.beta), 10)\n",
    "    x = list(scale * R * np.cos(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.cos(v.phi - np.pi / 2))\n",
    "    y = list(scale * R * np.sin(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.sin(v.phi - np.pi / 2))\n",
    "    z = list(scale * v.pz * c * t_values / (v.gamma * v.mass))\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "track_px, track_py, track_pz, track_charge = (\n",
    "    track_features[\"px\"],\n",
    "    track_features[\"py\"],\n",
    "    track_features[\"pz\"],\n",
    "    track_features[\"q\"],\n",
    ")\n",
    "\n",
    "track_mass = np.zeros_like(track_px) + pion_mass\n",
    "\n",
    "track_x, track_y, track_z = [], [], []\n",
    "for irow in range(len(track_px)):\n",
    "\n",
    "    # convert to vector\n",
    "    v = vector.obj(px=track_px[irow], py=track_py[irow], pz=track_pz[irow], mass=track_mass[irow])\n",
    "\n",
    "    x, y, z = helix_eq(track_charge[irow], B, v)\n",
    "    track_x += x\n",
    "    track_y += y\n",
    "    track_z += z\n",
    "\n",
    "    track_x += [None]  # Add None to separate tracks in the plot\n",
    "    track_y += [None]\n",
    "    track_z += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"Raw ECAL hit\", 1: \"Raw HCAL hit\", 2: \"Raw Muon chamber hit\", 3: \"Raw tracker hit\"}\n",
    "\n",
    "subdetector_color = {0: \"steelblue\", 1: \"green\", 2: \"orange\", 3: \"red\"}  # ECAL  # HCAL  # MUON  # Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "# raw hits\n",
    "for subdetector in [0, 1, 2, 3]:\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=np.clip(df[\"px\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        y=np.clip(df[\"py\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        z=np.clip(df[\"pz\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=np.clip(2 + 2 * np.log(df[\"plotsize\"]), 1, 15),\n",
    "            color=subdetector_color[subdetector],\n",
    "            colorscale=\"Viridis\",\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name=labels[subdetector],\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# add the tracks\n",
    "trace = go.Scatter3d(\n",
    "    x=np.array(track_x),\n",
    "    y=np.array(track_y),\n",
    "    z=np.array(track_z),\n",
    "    mode=\"lines\",\n",
    "    name=\"Track\",\n",
    "    line=dict(color=\"red\"),\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# # add the clusters\n",
    "trace = go.Scatter3d(\n",
    "    x=cluster_features[\"position.x\"],\n",
    "    y=cluster_features[\"position.y\"],\n",
    "    z=cluster_features[\"position.z\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=cluster_features[\"energy\"],\n",
    "        color=\"blue\",\n",
    "        opacity=0.8,\n",
    "    ),\n",
    "    name=\"ECAL/HCAL clusters\",\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    legend=dict(x=0.8, y=0.5, font=dict(size=16)),  # https://plotly.com/python/legend/\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # for plotly to avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_tracks_rawcalohits.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg_dict = {\n",
    "    22: \"photon\",  # photon\n",
    "    11: \"electron\",  # electron\n",
    "    13: \"muon\",  # muon\n",
    "    130: \"n. hadron\",  # neutral hadron\n",
    "    211: \"ch. hadron\",  # charged hadron\n",
    "}\n",
    "\n",
    "color_dict = {\n",
    "    \"photon\": \"red\",  # photon\n",
    "    \"electron\": \"green\",  # electron\n",
    "    \"muon\": \"purple\",  # muon\n",
    "    \"n. hadron\": \"orange\",  # neutral hadron\n",
    "    \"ch. hadron\": \"blue\",  # charged hadron\n",
    "    None: \"black\",  # placeholder for skipped element\n",
    "}\n",
    "\n",
    "# Extract relevant data from gen_features\n",
    "gen_px = gen_features[\"px\"]\n",
    "gen_py = gen_features[\"py\"]\n",
    "gen_pz = gen_features[\"pz\"]\n",
    "gen_mass = gen_features[\"mass\"]\n",
    "gen_charge = gen_features[\"charge\"]\n",
    "gen_pdg = ak.to_numpy(np.absolute(gen_features[\"PDG\"]))\n",
    "\n",
    "# Set all other particles to ch.had or n.had\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & ak.to_numpy(np.abs(gen_charge) > 0)] = (\n",
    "    211  # when not filtering genstatus==1, charge can be between 0 and 1\n",
    ")\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & ak.to_numpy(np.abs(gen_charge) == 0)] = 130\n",
    "\n",
    "# Extrapolate MC particle trajectories\n",
    "mc_x = []\n",
    "mc_y = []\n",
    "mc_z = []\n",
    "pdg_list = []\n",
    "for irow in range(len(gen_px)):\n",
    "\n",
    "    # Convert to vector\n",
    "    v = vector.obj(px=gen_px[irow], py=gen_py[irow], pz=gen_pz[irow], mass=gen_mass[irow])\n",
    "    if gen_charge[irow] == 0:\n",
    "        this_mc_x = [0, np.clip(scale * v.px / v.mag, -4000, 4000)]\n",
    "        this_mc_y = [0, np.clip(scale * v.py / v.mag, -4000, 4000)]\n",
    "        this_mc_z = [0, np.clip(scale * v.pz / v.mag, -4000, 4000)]\n",
    "    else:\n",
    "        x, y, z = helix_eq(gen_charge[irow], B, v)\n",
    "        this_mc_x = x\n",
    "        this_mc_y = y\n",
    "        this_mc_z = z\n",
    "\n",
    "    pdg_list += len(this_mc_x) * [pdg_dict[gen_pdg[irow]]]\n",
    "\n",
    "    mc_x += this_mc_x\n",
    "    mc_y += this_mc_y\n",
    "    mc_z += this_mc_z\n",
    "\n",
    "    mc_x += [None]\n",
    "    mc_y += [None]\n",
    "    mc_z += [None]\n",
    "    pdg_list += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D scatter plot with one trace per particle\n",
    "traces = []\n",
    "unique_particles = set(pdg_list)  # Get unique particle types\n",
    "\n",
    "for particle in unique_particles:\n",
    "\n",
    "    # Get indices for the current particle, including None at the end of each track\n",
    "    indices = []\n",
    "    for i, p in enumerate(pdg_list):\n",
    "        if p is None:  # we don't need to add non-particles\n",
    "            continue\n",
    "        if p == particle:\n",
    "            indices.append(i)\n",
    "            if pdg_list[i + 1] is None:\n",
    "                indices.append(i + 1)  # Add None at the end of the track to separate tracks in plot\n",
    "\n",
    "    # Create a separate trace for each particle\n",
    "    traces.append(\n",
    "        go.Scatter3d(\n",
    "            x=np.array(mc_x)[indices],\n",
    "            y=np.array(mc_y)[indices],\n",
    "            z=np.array(mc_z)[indices],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color_dict[particle]),  # Assign color for the particle\n",
    "            name=f\"{particle}\",  # Add particle name to the legend\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # Avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_particles_legend.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the alignment of extrapolated tracks and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for itrk in range(9):\n",
    "    plt.sca(axes[itrk])\n",
    "    v = vector.obj(px=track_px[itrk], py=track_py[itrk], pz=track_pz[itrk], mass=track_mass[itrk])\n",
    "\n",
    "    # Get the global hit indices associated with the current track\n",
    "    # track_to_tracker_hit_matrix is a tuple where the first element contains the track indices and the second element contains the hit indices\n",
    "    hit_indices = track_to_tracker_hit_matrix[\"hit_idx\"][track_to_tracker_hit_matrix[\"track_idx\"] == itrk]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = tracker_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_z = tracker_hit_features[\"position.z\"][hit_indices]\n",
    "\n",
    "    # Calculate the helix trajectory\n",
    "    x, y, z = helix_eq(track_charge[itrk], B, v)\n",
    "\n",
    "    # Plot the track and associated hits\n",
    "    plt.plot(x, z, label=\"Track\")\n",
    "    plt.scatter(hs_x, hs_z, color=\"red\", marker=\".\", label=\"Tracker hit\")\n",
    "    plt.xlim(-2000, 2000)\n",
    "    plt.ylim(-2000, 2000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[itrk].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the alignment of clusters and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for icls in range(9):\n",
    "    plt.sca(axes[icls])\n",
    "\n",
    "    # Get the global hit indices associated with the current cluster\n",
    "    # cluster_to_cluster_hit_matrix is a tuple where the first element contains the cluster indices and the second element contains the hit indices\n",
    "    hit_indices = cluster_to_cluster_hit_matrix[\"hit_idx\"][cluster_to_cluster_hit_matrix[\"cluster_idx\"] == icls]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = calo_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_y = calo_hit_features[\"position.y\"][hit_indices]\n",
    "\n",
    "    # Plot the cluster and associated hits\n",
    "    plt.scatter(\n",
    "        cluster_features[\"position.x\"][icls],\n",
    "        cluster_features[\"position.y\"][icls],\n",
    "        s=100 * cluster_features[\"energy\"][icls],\n",
    "        alpha=0.5,\n",
    "        label=\"cluster\",\n",
    "    )\n",
    "    plt.scatter(hs_x, hs_y, color=\"red\", marker=\".\", label=\"hit\")\n",
    "    plt.xlim(-4000, 4000)\n",
    "    plt.ylim(-4000, 4000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[icls].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genparticle to calorimeter hit associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract genparticle_to_calo_hit_matrix from event_data1\n",
    "# This matrix contains the mapping of genparticles to calorimeter hits in a COO format\n",
    "rows = genparticle_to_calo_hit_matrix[\"gen_idx\"]\n",
    "cols = genparticle_to_calo_hit_matrix[\"hit_idx\"]\n",
    "weights = genparticle_to_calo_hit_matrix[\"weight\"]\n",
    "\n",
    "# create dense coo amtrix\n",
    "gp_to_calo_hit_matrix = coo_matrix((weights, (rows, cols)), shape=(np.max(rows) + 1, np.max(cols) + 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights), np.sum(gp_to_calo_hit_matrix > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many genparticles leave at least one hit? (we haven't filtered on generatorStatus yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rows that sum to greater than 0\n",
    "non_zero_rows = np.where(np.sum(gp_to_calo_hit_matrix, axis=1) > 0)[0]\n",
    "\n",
    "# Create a new matrix with the extracted rows\n",
    "gp_to_calo_hit_matrix_non_zero = gp_to_calo_hit_matrix[non_zero_rows, :]\n",
    "gp_to_calo_hit_matrix.shape, gp_to_calo_hit_matrix_non_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of genparticles: {gp_to_calo_hit_matrix.shape[0]}\")\n",
    "print(f\"Number of genparticles with > 0 hits: {len(non_zero_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many genparticles leave more than 1 hit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows with more than 1 element with weight > 0\n",
    "rows_with_multiple_elements = np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=1) > 1)\n",
    "print(f\"Number of genparticles leaving more than 1 hit: {rows_with_multiple_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hits are assoicated to more than one genparticle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count cols with more than one element > 0\n",
    "cols_with_multiple_elements = np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=0) > 1)\n",
    "\n",
    "# Extract rows that sum to greater than 0\n",
    "multiple_gp_per_hit_mask = np.where(np.sum(gp_to_calo_hit_matrix > 0, axis=0) > 1)\n",
    "\n",
    "# Create a new matrix with the extracted rows\n",
    "multiple_gp_per_hit = gp_to_calo_hit_matrix[:, multiple_gp_per_hit_mask[1]]\n",
    "multiple_gp_per_hit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of hits with multiple genparticles: {multiple_gp_per_hit.shape[1]}\")\n",
    "print(f\"Number of hits with multiple genparticles: {cols_with_multiple_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of the hits associated to multiple genparticles, how many genparticles are each hit associated to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(multiple_gp_per_hit > 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_hit in range(multiple_gp_per_hit.shape[1]):\n",
    "    gp_links = multiple_gp_per_hit[:, i_hit]\n",
    "    gp_link_mask = gp_links > 0\n",
    "    gp_links = gp_links[gp_link_mask]\n",
    "    print(f\"Hit {i_hit} is linked to genparticles: {np.where(gp_link_mask)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_root_files_to_parquet(input_dir, output_dir, max_root_files=None):\n",
    "    \"\"\"\n",
    "    Process ROOT files and save extracted features and matrices into Parquet files using ak arrays.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): Directory containing ROOT files.\n",
    "        output_dir (str or Path): Directory to save Parquet files.\n",
    "        max_root_files (int, optional): Maximum number of ROOT files to process. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    root_counter = 0\n",
    "    root_file_list = list(Path(input_dir).rglob(\"*.root\"))\n",
    "    total_files_to_porcess = max_root_files or len(root_file_list)\n",
    "\n",
    "    for root_file in tqdm(sorted(root_file_list), desc=\"Processing ROOT files\", total=total_files_to_porcess):\n",
    "        if max_root_files is not None and root_counter >= max_root_files:\n",
    "            print(f\"Reached max_root_files limit: {max_root_files}. Stopping processing.\")\n",
    "            break\n",
    "        try:\n",
    "            output_file = output_dir / f\"{root_file.stem}.parquet\"\n",
    "            if output_file.exists():\n",
    "                print(f\"Output file {output_file} already exists. Skipping processing.\")\n",
    "                return\n",
    "\n",
    "            fi = uproot.open(root_file)\n",
    "            collectionIDs = {\n",
    "                k: v\n",
    "                for k, v in zip(\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\n",
    "                        \"events___idTable/m_collectionIDs\"\n",
    "                    ][0],\n",
    "                )\n",
    "            }\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "\n",
    "            # Combine all events in the current ROOT file\n",
    "            combined_data_dict = {\n",
    "                \"gen_features\": [],\n",
    "                \"track_features\": [],\n",
    "                \"cluster_features\": [],\n",
    "                \"calo_hit_features\": [],\n",
    "                \"tracker_hit_features\": [],\n",
    "                \"genparticle_to_calo_hit_matrix\": [],\n",
    "                \"genparticle_to_tracker_hit_matrix\": [],\n",
    "                \"track_to_tracker_hit_matrix\": [],\n",
    "                \"cluster_to_cluster_hit_matrix\": [],\n",
    "                \"gp_to_track_matrix\": [],\n",
    "                \"gp_to_gp\": [],\n",
    "            }\n",
    "\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Append the event data to the combined data dictionary\n",
    "                combined_data_dict[\"gen_features\"].append(gen_features)\n",
    "                combined_data_dict[\"track_features\"].append(track_features)\n",
    "                combined_data_dict[\"cluster_features\"].append(cluster_features)\n",
    "                combined_data_dict[\"calo_hit_features\"].append(calo_hit_features)\n",
    "                combined_data_dict[\"tracker_hit_features\"].append(tracker_hit_features)\n",
    "                combined_data_dict[\"genparticle_to_calo_hit_matrix\"].append(genparticle_to_calo_hit_matrix)\n",
    "                combined_data_dict[\"genparticle_to_tracker_hit_matrix\"].append(genparticle_to_tracker_hit_matrix)\n",
    "                combined_data_dict[\"track_to_tracker_hit_matrix\"].append(track_to_tracker_hit_matrix)\n",
    "                combined_data_dict[\"cluster_to_cluster_hit_matrix\"].append(cluster_to_cluster_hit_matrix)\n",
    "                combined_data_dict[\"gp_to_track_matrix\"].append(gp_to_track_matrix)\n",
    "                combined_data_dict[\"gp_to_gp\"].append(gp_to_gp)\n",
    "\n",
    "            # Convert lists to ak arrays\n",
    "            for key in combined_data_dict.keys():\n",
    "                combined_data_dict[key] = ak.Array(combined_data_dict[key])\n",
    "\n",
    "            # Save the combined data into a single Parquet file\n",
    "            ak.to_parquet(combined_data_dict, output_file)\n",
    "\n",
    "            print(f\"Saved combined data for {root_file} to {output_file}\")\n",
    "            root_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    print(f\"Finished processing {root_counter} ROOT files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_root_files_to_parquet(\"/mnt/ceph/users/ewulff/data/cld/\", \"/mnt/ceph/users/ewulff/data/cld/processed/parquet\", max_root_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data1 = ak.from_parquet(next(Path(\"/mnt/ceph/users/ewulff/data/cld/processed/parquet/\").glob(\"*.parquet\")))\n",
    "event_data1.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumps(obj):\n",
    "    \"\"\"\n",
    "    Serialize an object.\n",
    "\n",
    "    Returns:\n",
    "        Implementation-dependent bytes-like object\n",
    "    \"\"\"\n",
    "    return pickle.dumps(obj, protocol=5)\n",
    "\n",
    "\n",
    "def process_root_files_to_lmdb(input_dir, output, max_root_files=None):\n",
    "    \"\"\"\n",
    "    Process ROOT files and save extracted features into an LMDB database.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): Directory containing ROOT files.\n",
    "        output (str or Path): Path to the LMDB database file.\n",
    "        collectionIDs (dict): Mapping of collection names to their IDs.\n",
    "        max_root_files (int, optional): Maximum number of ROOT files to process. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    lmdb_path = os.path.expanduser(output)\n",
    "    isdir = os.path.isdir(lmdb_path)\n",
    "\n",
    "    print(\"Generate LMDB to %s\" % lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=isdir, map_size=1099511627776 * 2, readonly=False, meminit=False, map_async=True)\n",
    "\n",
    "    txn = db.begin(write=True)\n",
    "    event_counter = 0\n",
    "    root_counter = 0\n",
    "\n",
    "    for root_file in tqdm(Path(input_dir).rglob(\"*.root\"), desc=\"Processing ROOT files\"):\n",
    "        if max_root_files is not None and root_counter >= max_root_files:\n",
    "            print(f\"Reached max_root_files limit: {max_root_files}. Stopping processing.\")\n",
    "            break\n",
    "        try:\n",
    "            fi = uproot.open(root_file)\n",
    "            collectionIDs = {\n",
    "                k: v\n",
    "                for k, v in zip(\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\n",
    "                        \"events___idTable/m_collectionIDs\"\n",
    "                    ][0],\n",
    "                )\n",
    "            }\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Save the event data into LMDB\n",
    "                txn.put(\n",
    "                    \"{}\".format(event_counter).encode(\"ascii\"),\n",
    "                    dumps(\n",
    "                        {\n",
    "                            \"gen_features\": gen_features,\n",
    "                            \"track_features\": track_features,\n",
    "                            \"cluster_features\": cluster_features,\n",
    "                            \"calo_hit_features\": calo_hit_features,\n",
    "                            \"tracker_hit_features\": tracker_hit_features,\n",
    "                            \"genparticle_to_calo_hit_matrix\": genparticle_to_calo_hit_matrix,\n",
    "                            \"genparticle_to_tracker_hit_matrix\": genparticle_to_tracker_hit_matrix,\n",
    "                            \"track_to_tracker_hit_matrix\": track_to_tracker_hit_matrix,\n",
    "                            \"cluster_to_cluster_hit_matrix\": cluster_to_cluster_hit_matrix,\n",
    "                            \"gp_to_track_matrix\": gp_to_track_matrix,\n",
    "                            \"gp_to_gp\": gp_to_gp,\n",
    "                        }\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                if event_counter % 100 == 0:\n",
    "                    print(f\"[{event_counter}] events processed\")\n",
    "                    txn.commit()\n",
    "                    txn = db.begin(write=True)\n",
    "\n",
    "                event_counter += 1\n",
    "\n",
    "            root_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    # Finish iterating through all events\n",
    "    txn.commit()\n",
    "    keys = [\"{}\".format(k).encode(\"ascii\") for k in range(event_counter)]\n",
    "    with db.begin(write=True) as txn:\n",
    "        txn.put(b\"__keys__\", dumps(keys))\n",
    "        txn.put(b\"__len__\", dumps(len(keys)))\n",
    "\n",
    "    print(\"Flushing database ...\")\n",
    "    db.sync()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_root_files_to_lmdb(\n",
    "#     input_dir=\"/mnt/ceph/users/ewulff/data/cld/\", output=\"/mnt/ceph/users/ewulff/data/cld/processed/lmdb\", max_root_files=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lmdb database\n",
    "def read_full_lmdb_database(lmdb_path):\n",
    "    lmdb_path = os.path.expanduser(lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=os.path.isdir(lmdb_path), readonly=True, lock=False)\n",
    "\n",
    "    with db.begin() as txn:\n",
    "        keys = pickle.loads(txn.get(b\"__keys__\"))\n",
    "        data = {ii: pickle.loads(txn.get(key)) for ii, key in enumerate(keys)}\n",
    "\n",
    "    db.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "# lmdb_data = read_full_lmdb_database(\"/mnt/ceph/users/ewulff/data/cld/processed/lmdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML task\n",
    "- Set up clustering (and tracking?) as instance segmentation problems\n",
    "- Instance segmentation labels are based on hits-to-genparticle associations in \n",
    "    - genparticle_to_calo_hit_matrix (and genparticle_to_tracker_hit_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data1 = ak.from_parquet(next(Path(\"/mnt/ceph/users/ewulff/data/cld/processed/parquet/\").glob(\"*.parquet\")))\n",
    "\n",
    "# Extract genparticle_to_calo_hit_matrix from event_data1\n",
    "# This matrix contains the mapping of genparticles to calorimeter hits in a COO format\n",
    "event_i = 2\n",
    "gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\n",
    "    \"weight\"\n",
    "].to_numpy()  # only contains the non-zero weights\n",
    "\n",
    "# create dense coo amtrix\n",
    "# gp_to_calo_hit_matrix = coo_matrix((weights, (rows, cols)), shape=(np.max(rows) + 1, np.max(cols) + 1)).todense()\n",
    "\n",
    "# Create instance segmentation-like labels for each hit\n",
    "# Each hit is classified as belonging to one genparticle based on the highest weight\n",
    "\n",
    "\n",
    "def get_hit_labels(hit_idx, gen_idx, weights):\n",
    "    \"\"\"\n",
    "    Assign labels to hits based on the genparticle index with the highest weight.\n",
    "\n",
    "    Parameters:\n",
    "        hit_idx (np.ndarray): Array of hit indices.\n",
    "        gen_idx (np.ndarray): Array of genparticle indices corresponding to each hit.\n",
    "        weights (np.ndarray): Array of weights corresponding to each hit.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of labels for each hit, where each label corresponds to the genparticle index.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store labels for each hit\n",
    "    hit_labels = np.full(np.max(hit_idx) + 1, -1, dtype=int)  # Default label is -1 (unclassified)\n",
    "\n",
    "    # Iterate through the sparse COO matrix data\n",
    "    for ii, (h_idx, g_idx, weight) in enumerate(zip(hit_idx, gen_idx, weights)):\n",
    "        if hit_labels[h_idx] == -1 or weight > weights[ii]:\n",
    "            hit_labels[h_idx] = g_idx\n",
    "\n",
    "    # hit_labels now contains the genparticle index for each hit\n",
    "\n",
    "    return hit_labels\n",
    "\n",
    "\n",
    "hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "hit_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data for testing\n",
    "def create_mock_event_data():\n",
    "    return ak.Array(\n",
    "        {\n",
    "            \"genparticle_to_calo_hit_matrix\": [\n",
    "                {\n",
    "                    \"gen_idx\": np.array([0, 1, 0, 2]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.5, 0.8, 0.3, 0.9]),\n",
    "                },\n",
    "                {\n",
    "                    \"gen_idx\": np.array([1, 2, 1, 0]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.6, 0.7, 0.4, 0.2]),\n",
    "                },\n",
    "                {\n",
    "                    \"gen_idx\": np.array([0, 2, 1, 2]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.9, 0.5, 0.7, 0.8]),\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_hit_labels():\n",
    "    event_data1 = create_mock_event_data()\n",
    "    event_i = 2\n",
    "    gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "    hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "    weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"weight\"].to_numpy()\n",
    "\n",
    "    hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "\n",
    "    # Expected labels based on mock data\n",
    "    expected_labels = np.array([0, 2, 1, 2])\n",
    "\n",
    "    assert np.array_equal(hit_labels, expected_labels), f\"Expected {expected_labels}, but got {hit_labels}\"\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_hit_labels()\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\n",
    "    \"weight\"\n",
    "].to_numpy()  # only contains the non-zero weights\n",
    "calo_hit_features = event_data1[\"calo_hit_features\"][event_i]\n",
    "\n",
    "# Assign labels to hits based on the genparticle index with the highest weight\n",
    "hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "\n",
    "# Extract calorimeter hit positions (x, y, z)\n",
    "calo_hit_positions = np.column_stack(\n",
    "    (\n",
    "        calo_hit_features[\"position.x\"].to_numpy(),\n",
    "        calo_hit_features[\"position.y\"].to_numpy(),\n",
    "        calo_hit_features[\"position.z\"].to_numpy(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Assign unique colors to each genparticle ID\n",
    "unique_ids = np.unique(hit_labels)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_ids)))\n",
    "color_map = {\n",
    "    gen_id: f\"rgba({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)}, {color[3]})\"\n",
    "    for gen_id, color in zip(unique_ids, colors)\n",
    "}\n",
    "\n",
    "# Create traces for each genparticle ID\n",
    "traces = []\n",
    "for gen_id in unique_ids:\n",
    "    mask = hit_labels == gen_id  # Create a mask for hits belonging to the current genparticle ID\n",
    "    traces.append(\n",
    "        go.Scatter3d(\n",
    "            x=calo_hit_positions[mask, 0],\n",
    "            y=calo_hit_positions[mask, 1],\n",
    "            z=calo_hit_positions[mask, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=color_map[gen_id]),\n",
    "            name=f\"gp {gen_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"X\"),\n",
    "        yaxis=dict(title=\"Y\"),\n",
    "        zaxis=dict(title=\"Z\"),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.1),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title=\"Calorimeter hits colored by genparticle\",\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genparticles(event_data, iev):\n",
    "    \"\"\"\n",
    "    Identify genparticles that leave more than a specified number of hits in the calorimeter.\n",
    "\n",
    "    Args:\n",
    "        gp_to_calo_hit_matrix (numpy.ndarray): The matrix representing the mapping of genparticles to calorimeter hits.\n",
    "        threshold (int): The minimum number of hits a genparticle must leave to be included in the result.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Indices of genparticles that leave more than the specified number of hits.\n",
    "    \"\"\"\n",
    "\n",
    "    gen_idx = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\"gen_idx\"].to_numpy()\n",
    "    hit_idx = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\"hit_idx\"].to_numpy()\n",
    "    weights = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\n",
    "        \"weight\"\n",
    "    ].to_numpy()  # only contains the non-zero weights\n",
    "\n",
    "    # create dense coo amtrix\n",
    "    gp_to_calo_hit_matrix = coo_matrix(\n",
    "        (weights, (gen_idx, hit_idx)), shape=(np.max(gen_idx) + 1, np.max(hit_idx) + 1)\n",
    "    ).todense()\n",
    "\n",
    "    # Extract rows that sum to greater than 0\n",
    "    non_zero_rows = np.where(np.sum(gp_to_calo_hit_matrix, axis=1) > 0)[0]\n",
    "\n",
    "    print(f\"Number of genparticles: {gp_to_calo_hit_matrix.shape[0]}\")\n",
    "    print(f\"Number of genparticles with > 0 hits: {len(non_zero_rows)}\")\n",
    "    print(f\"Number of genparticles with > 1 hits: {np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=1) > 1)}\")\n",
    "\n",
    "\n",
    "count_genparticles(event_data1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
