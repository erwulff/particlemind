import math
import numpy as np
import torch
import torch.nn as nn
from torch.nn.attention import SDPBackend, sdpa_kernel


def get_activation(activation):
    if activation == "elu":
        act = nn.ELU
    elif activation == "relu":
        act = nn.ReLU
    elif activation == "relu6":
        act = nn.ReLU6
    elif activation == "leakyrelu":
        act = nn.LeakyReLU
    elif activation == "gelu":
        act = nn.GELU
    return act


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # From https://github.com/rwightman/pytorch-image-models/blob/
    #        18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """

    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lo = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2lo-1, 2up-1].
        tensor.uniform_(2 * lo - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


class PreLnSelfAttentionLayer(nn.Module):
    def __init__(
        self,
        name="",
        activation="elu",
        embedding_dim=128,
        num_heads=2,
        width=128,
        dropout_mha=0.1,
        dropout_ff=0.1,
        attention_type="flash",
        learnable_queries=False,
        elems_as_queries=False,
    ):
        super(PreLnSelfAttentionLayer, self).__init__()
        self.name = name

        # set to False to enable manual override for ONNX export
        self.enable_ctx_manager = True

        self.attention_type = attention_type
        self.act = get_activation(activation)
        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)
        self.norm0 = torch.nn.LayerNorm(embedding_dim)
        self.norm1 = torch.nn.LayerNorm(embedding_dim)
        self.seq = torch.nn.Sequential(
            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()
        )
        self.dropout = torch.nn.Dropout(dropout_ff)

        # params for torch sdp_kernel
        if self.enable_ctx_manager:
            self.attn_params = {
                "math": [SDPBackend.MATH],
                "efficient": [SDPBackend.EFFICIENT_ATTENTION],
                "flash": [SDPBackend.FLASH_ATTENTION],
            }

        self.learnable_queries = learnable_queries
        self.elems_as_queries = elems_as_queries
        if self.learnable_queries:
            self.queries = nn.Parameter(torch.zeros(1, 1, embedding_dim), requires_grad=True)
            trunc_normal_(self.queries, std=0.02)

        self.save_attention = False
        self.outdir = ""

    def forward(self, x, mask, initial_embedding):
        mask_ = mask.unsqueeze(-1)
        x = self.norm0(x * mask_)

        q = x
        if self.learnable_queries:
            q = self.queries.expand(*x.shape) * mask_
        elif self.elems_as_queries:
            q = initial_embedding * mask_

        key_padding_mask = None
        if self.attention_type == "math":
            key_padding_mask = ~mask

        # default path, for FlashAttn/Math backend
        if self.enable_ctx_manager:
            with sdpa_kernel(self.attn_params[self.attention_type]):
                mha_out = self.mha(q, x, x, need_weights=False, key_padding_mask=key_padding_mask)[0]

                if self.save_attention:
                    att_mat = self.mha(q, x, x, need_weights=True, key_padding_mask=key_padding_mask)[1]
                    att_mat = att_mat.detach().cpu().numpy()
                    np.savez(
                        open("{}/attn_{}.npz".format(self.outdir, self.name), "wb"),
                        att=att_mat,
                        in_proj_weight=self.mha.in_proj_weight.detach().cpu().numpy(),
                    )

        # path for ONNX export
        else:
            mha_out = self.mha(q, x, x, need_weights=False, key_padding_mask=key_padding_mask)[0]

        mha_out = mha_out * mask_

        mha_out = x + mha_out
        x = self.norm1(mha_out)
        x = mha_out + self.seq(x)
        x = self.dropout(x)
        x = x * mask_
        return x


class InputEncoder(nn.Module):
    def __init__(self, input_size, embed_dim, dropout=0.1):
        super(InputEncoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, embed_dim),
            nn.LayerNorm(embed_dim),
            get_activation("elu")(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
        )

    def forward(self, x):
        return self.encoder(x)


class SelfAttentionTransformer(nn.Module):
    def __init__(
        self, input_size, output_size, embed_dim, num_layers, num_heads, ff_dim, dropout=0.1, attention_type="math"
    ):
        super(SelfAttentionTransformer, self).__init__()

        self.input_encoder = InputEncoder(input_size, embed_dim, dropout)

        self.layers = nn.ModuleList(
            [
                PreLnSelfAttentionLayer(
                    activation="elu",
                    embedding_dim=embed_dim,
                    num_heads=num_heads,
                    width=ff_dim,
                    dropout_mha=dropout,
                    dropout_ff=dropout,
                    attention_type=attention_type,
                    learnable_queries=False,
                    elems_as_queries=False,
                )
                for _ in range(num_layers)
            ]
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.fc_out = nn.Linear(embed_dim, output_size)

    def forward(self, x_orig, mask=None):
        if mask is None:
            mask = torch.ones(x_orig.shape[0], x_orig.shape[1], dtype=torch.bool, device=x_orig.device)
        # Embedding and positional encoding
        x = self.input_encoder(x_orig)

        embeddings = []
        for num, layer in enumerate(self.layers):
            layer_input = x if num == 0 else embeddings[-1]
            out_padded = layer(layer_input, mask, x_orig)
            embeddings.append(out_padded)

        final_embedding = embeddings[-1]

        # Final normalization and output projection
        x = self.norm(final_embedding)
        x = self.fc_out(x)
        return x
