{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for CLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uproot\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas\n",
    "import awkward\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import lmdb\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import vector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "These are defined in `data_processing/cld_processing` but are reproduced here clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "pion_mass = 0.13957\n",
    "B = -2.0  # magnetic field in T (-2 for CLD FCC-ee)\n",
    "c = 3e8  # speed of light in m/s\n",
    "scale = 1000\n",
    "\n",
    "\n",
    "def get_sitrack_links(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"SiTracksMCTruthLink.weight\",\n",
    "            \"_SiTracksMCTruthLink_to/_SiTracksMCTruthLink_to.collectionID\",\n",
    "            \"_SiTracksMCTruthLink_to/_SiTracksMCTruthLink_to.index\",\n",
    "            \"_SiTracksMCTruthLink_from/_SiTracksMCTruthLink_from.collectionID\",\n",
    "            \"_SiTracksMCTruthLink_from/_SiTracksMCTruthLink_from.index\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tracker_hits_begin_end(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"SiTracks_Refitted/SiTracks_Refitted.trackerHits_begin\",\n",
    "            \"SiTracks_Refitted/SiTracks_Refitted.trackerHits_end\",\n",
    "            \"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.index\",\n",
    "            \"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.collectionID\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cluster_hits_begin_end(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"PandoraClusters/PandoraClusters.hits_begin\",\n",
    "            \"PandoraClusters/PandoraClusters.hits_end\",\n",
    "            \"_PandoraClusters_hits/_PandoraClusters_hits.index\",\n",
    "            \"_PandoraClusters_hits/_PandoraClusters_hits.collectionID\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_calohit_links(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"CalohitMCTruthLink.weight\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.collectionID\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.index\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.collectionID\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.index\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_calo_hit_data(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"ECALBarrel\",\n",
    "            \"ECALEndcap\",\n",
    "            \"HCALBarrel\",\n",
    "            \"HCALEndcap\",\n",
    "            \"HCALOther\",\n",
    "            \"MUON\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tracker_hit_data(ev):\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            \"VXDTrackerHits\",\n",
    "            \"VXDEndcapTrackerHits\",\n",
    "            \"ITrackerHits\",\n",
    "            \"OTrackerHits\",\n",
    "            # These need to be added to the keep statements of the next generation\n",
    "            # \"ITrackerEndcapHits\",\n",
    "            # \"OTrackerEndcapHits\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_track_data(ev):\n",
    "    track_data = ev.arrays(\n",
    "        [\n",
    "            \"SiTracks_Refitted\",\n",
    "            \"SiTracks_Refitted_dQdx\",\n",
    "            \"_SiTracks_Refitted_trackStates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return track_data\n",
    "\n",
    "\n",
    "def get_cluster_data(ev):\n",
    "    cluster_data = ev.arrays(\n",
    "        [\n",
    "            \"PandoraClusters\",\n",
    "            \"_PandoraClusters_hits\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return cluster_data\n",
    "\n",
    "\n",
    "def get_gen_data(ev):\n",
    "    gen_data = ev.arrays([\"MCParticles\"])\n",
    "    return gen_data\n",
    "\n",
    "\n",
    "def get_event_data(ev):\n",
    "    \"\"\"\n",
    "    Retrieves all data entries returned by the existing functions using ev.arrays().\n",
    "\n",
    "    Args:\n",
    "        ev: The event data object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all data entries.\n",
    "    \"\"\"\n",
    "    return ev.arrays(\n",
    "        [\n",
    "            # SiTrack links\n",
    "            \"SiTracksMCTruthLink.weight\",\n",
    "            \"_SiTracksMCTruthLink_to/_SiTracksMCTruthLink_to.collectionID\",\n",
    "            \"_SiTracksMCTruthLink_to/_SiTracksMCTruthLink_to.index\",\n",
    "            \"_SiTracksMCTruthLink_from/_SiTracksMCTruthLink_from.collectionID\",\n",
    "            \"_SiTracksMCTruthLink_from/_SiTracksMCTruthLink_from.index\",\n",
    "            # Tracker hits begin/end\n",
    "            \"SiTracks_Refitted/SiTracks_Refitted.trackerHits_begin\",\n",
    "            \"SiTracks_Refitted/SiTracks_Refitted.trackerHits_end\",\n",
    "            \"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.index\",\n",
    "            \"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.collectionID\",\n",
    "            # Cluster hits begin/end\n",
    "            \"PandoraClusters/PandoraClusters.hits_begin\",\n",
    "            \"PandoraClusters/PandoraClusters.hits_end\",\n",
    "            \"_PandoraClusters_hits/_PandoraClusters_hits.index\",\n",
    "            \"_PandoraClusters_hits/_PandoraClusters_hits.collectionID\",\n",
    "            # Calo hit links\n",
    "            \"CalohitMCTruthLink.weight\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.collectionID\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.index\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.collectionID\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.index\",\n",
    "            # Calo hit data\n",
    "            \"ECALBarrel\",\n",
    "            \"ECALEndcap\",\n",
    "            \"HCALBarrel\",\n",
    "            \"HCALEndcap\",\n",
    "            \"HCALOther\",\n",
    "            \"MUON\",\n",
    "            # Tracker hit data\n",
    "            \"VXDTrackerHits\",\n",
    "            \"VXDEndcapTrackerHits\",\n",
    "            \"ITrackerHits\",\n",
    "            \"OTrackerHits\",\n",
    "            # Track data\n",
    "            \"SiTracks_Refitted\",\n",
    "            \"SiTracks_Refitted_dQdx\",\n",
    "            \"_SiTracks_Refitted_trackStates\",\n",
    "            # Cluster data\n",
    "            \"PandoraClusters\",\n",
    "            \"_PandoraClusters_hits\",\n",
    "            # Gen data\n",
    "            \"MCParticles\",\n",
    "            \"MCParticles.parents_begin\",\n",
    "            \"MCParticles.parents_end\",\n",
    "            \"_MCParticles_parents/_MCParticles_parents.index\",\n",
    "            \"MCParticles.daughters_begin\",\n",
    "            \"MCParticles.daughters_end\",\n",
    "            \"_MCParticles_daughters/_MCParticles_daughters.index\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def hits_to_features(hit_data, iev, coll, feats):\n",
    "    \"\"\"\n",
    "    Converts hit data into a structured feature array for a specific event and collection.\n",
    "\n",
    "    Args:\n",
    "        hit_data (dict): A dictionary containing hit data, where keys are strings representing\n",
    "            collection and feature names, and values are arrays of feature data.\n",
    "        iev (int): The index of the event to extract data for.\n",
    "        coll (str): The name of the hit collection (e.g., \"VXDTrackerHits\", \"VXDEndcapTrackerHits\",\n",
    "            \"ECALBarrel\", \"ECALEndcap\", etc.).\n",
    "        feats (list of str): A list of feature names to extract from the hit data\n",
    "            (e.g., \"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\", etc.).\n",
    "\n",
    "    Returns:\n",
    "        awkward.Array: An Awkward Array containing the extracted features for the specified event\n",
    "            and collection. The array includes an additional \"subdetector\" feature, which encodes\n",
    "            the subdetector type:\n",
    "            - 0 for ECAL\n",
    "            - 1 for HCAL\n",
    "            - 2 for MUON\n",
    "            - 3 for other collections.\n",
    "    \"\"\"\n",
    "    # tracker hits store eDep instead of energy\n",
    "    if \"TrackerHit\" in coll or \"TrackerEndcapHits\" in coll:\n",
    "        new_feats = []\n",
    "        for feat in feats:\n",
    "            feat_to_get = feat\n",
    "            if feat == \"energy\":\n",
    "                feat_to_get = \"eDep\"\n",
    "            new_feats.append((feat, feat_to_get))\n",
    "    else:\n",
    "        new_feats = [(f, f) for f in feats]\n",
    "\n",
    "    feat_arr = {f1: hit_data[coll + \".\" + f2][iev] for f1, f2 in new_feats}\n",
    "\n",
    "    sdcoll = \"subdetector\"\n",
    "    feat_arr[sdcoll] = np.zeros(len(feat_arr[\"type\"]), dtype=np.int32)\n",
    "    if coll.startswith(\"ECAL\"):\n",
    "        feat_arr[sdcoll][:] = 0\n",
    "    elif coll.startswith(\"HCAL\"):\n",
    "        feat_arr[sdcoll][:] = 1\n",
    "    elif coll.startswith(\"MUON\"):\n",
    "        feat_arr[sdcoll][:] = 2\n",
    "    else:\n",
    "        feat_arr[sdcoll][:] = 3\n",
    "    return awkward.Array(feat_arr)\n",
    "\n",
    "\n",
    "def genparticle_track_adj(event_data, iev):\n",
    "    trk_to_gen_trkidx = event_data[\"_SiTracksMCTruthLink_from/_SiTracksMCTruthLink_from.index\"][iev]\n",
    "    trk_to_gen_genidx = event_data[\"_SiTracksMCTruthLink_to/_SiTracksMCTruthLink_to.index\"][iev]\n",
    "    trk_to_gen_w = event_data[\"SiTracksMCTruthLink.weight\"][iev]\n",
    "\n",
    "    genparticle_to_track_matrix_coo0 = awkward.to_numpy(trk_to_gen_genidx)\n",
    "    genparticle_to_track_matrix_coo1 = awkward.to_numpy(trk_to_gen_trkidx)\n",
    "    genparticle_to_track_matrix_w = awkward.to_numpy(trk_to_gen_w)\n",
    "\n",
    "    return genparticle_to_track_matrix_coo0, genparticle_to_track_matrix_coo1, genparticle_to_track_matrix_w\n",
    "\n",
    "\n",
    "def produce_gp_to_track(ev, iev, num_genparticles, num_tracks):\n",
    "    \"\"\"\n",
    "    Produces the genparticle-to-track adjacency matrix (gp_to_track) in dense format.\n",
    "\n",
    "    Args:\n",
    "        iev: The event index.\n",
    "        num_genparticles: Total number of genparticles.\n",
    "        num_tracks: Total number of tracks.\n",
    "\n",
    "    Returns:\n",
    "        gp_to_track: A dense adjacency matrix where each entry represents the weight\n",
    "                     between a genparticle and a track.\n",
    "    \"\"\"\n",
    "    # Get the COO format adjacency data\n",
    "    genparticle_to_track_matrix_coo0, genparticle_to_track_matrix_coo1, genparticle_to_track_matrix_w = (\n",
    "        genparticle_track_adj(ev, iev)\n",
    "    )\n",
    "\n",
    "    # Create a sparse matrix for the association between gen particles and tracks\n",
    "    if len(genparticle_to_track_matrix_coo0) > 0:\n",
    "        gp_to_track = coo_matrix(\n",
    "            (genparticle_to_track_matrix_w, (genparticle_to_track_matrix_coo0, genparticle_to_track_matrix_coo1)),\n",
    "            shape=(num_genparticles, num_tracks),\n",
    "        ).todense()\n",
    "    else:\n",
    "        gp_to_track = np.zeros((num_genparticles, 1))\n",
    "\n",
    "    return gp_to_track\n",
    "\n",
    "\n",
    "def create_global_to_local_mapping(hit_data, iev, collectionIDs):\n",
    "    \"\"\"Create a mapping from global hit indices to local (collection, index) pairs.\"\"\"\n",
    "    hit_idx_global_to_local = {}\n",
    "    hit_idx_global = 0\n",
    "\n",
    "    for col in sorted(hit_data.fields):\n",
    "        icol = collectionIDs[col]\n",
    "        for ihit in range(len(hit_data[col][col + \".position.x\"][iev])):\n",
    "            hit_idx_global_to_local[hit_idx_global] = (icol, ihit)\n",
    "            hit_idx_global += 1\n",
    "\n",
    "    hit_idx_local_to_global = {v: k for k, v in hit_idx_global_to_local.items()}\n",
    "\n",
    "    return hit_idx_global_to_local, hit_idx_local_to_global\n",
    "\n",
    "\n",
    "def create_hit_feature_matrix(hit_data, iev, feats):\n",
    "    \"\"\"Extract features from hit data and create a feature matrix.\"\"\"\n",
    "    hit_feature_matrix = []\n",
    "\n",
    "    for col in sorted(hit_data.fields):\n",
    "        hit_features = hits_to_features(hit_data[col], iev, col, feats)\n",
    "        hit_feature_matrix.append(hit_features)\n",
    "\n",
    "    # Combine all hit features into a single Record\n",
    "    hit_feature_matrix = {\n",
    "        k: awkward.concatenate([hit_feature_matrix[i][k] for i in range(len(hit_feature_matrix))])\n",
    "        for k in hit_feature_matrix[0].fields\n",
    "    }\n",
    "\n",
    "    return hit_feature_matrix\n",
    "\n",
    "\n",
    "# Combine the two functions above into one to reduce the number of loops over hit data.\n",
    "def create_hit_feature_matrix_and_mapping(hit_data, iev, collectionIDs, feats):\n",
    "    \"\"\"Combines the creation of hit feature matrix and global-local index mapping in one loop over hit data.\"\"\"\n",
    "\n",
    "    # Initialize global hit index mapping\n",
    "    hit_idx_global = 0\n",
    "    hit_idx_global_to_local = {}\n",
    "    hit_feature_matrix = []\n",
    "\n",
    "    # Process hit data to create feature matrix and global-local mappings\n",
    "    for col in sorted(hit_data.fields):\n",
    "        icol = collectionIDs[col]\n",
    "        hit_features = hits_to_features(hit_data[col], iev, col, feats)\n",
    "        hit_feature_matrix.append(hit_features)\n",
    "        for ihit in range(len(hit_data[col][col + \".position.x\"][iev])):\n",
    "            hit_idx_global_to_local[hit_idx_global] = (icol, ihit)\n",
    "            hit_idx_global += 1\n",
    "    hit_idx_local_to_global = {v: k for k, v in hit_idx_global_to_local.items()}\n",
    "    hit_feature_matrix = {\n",
    "        k: np.concatenate([hit_feature_matrix[i][k].to_numpy() for i in range(len(hit_feature_matrix))])\n",
    "        for k in hit_feature_matrix[0].fields\n",
    "    }\n",
    "\n",
    "    return hit_feature_matrix, hit_idx_global_to_local, hit_idx_local_to_global\n",
    "\n",
    "\n",
    "# TODO: check correctnes of this function\n",
    "def create_track_to_hit_coo_matrix(event_data, iev, collectionIDs):\n",
    "    \"\"\"\n",
    "    Creates the COO matrix indices and weights for the relationship between tracks and tracker hits.\n",
    "\n",
    "    Args:\n",
    "        ev: The event data containing track property data.\n",
    "        iev: The index of the event to extract data for.\n",
    "        collectionIDs: A dictionary mapping collection names to their IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three arrays:\n",
    "            - Row indices (track indices).\n",
    "            - Column indices (global hit indices).\n",
    "            - Weights (association weights between tracks and hits).\n",
    "    \"\"\"\n",
    "    # Extract tracker hit data\n",
    "    tracker_hit_data = event_data[\n",
    "        [\n",
    "            \"VXDTrackerHits\",\n",
    "            \"VXDEndcapTrackerHits\",\n",
    "            \"ITrackerHits\",\n",
    "            \"OTrackerHits\",\n",
    "            # \"ITrackerEndcapHits\",\n",
    "            # \"OTrackerEndcapHits\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Extract tracker hit to track associations\n",
    "    hit_beg = event_data[\"SiTracks_Refitted/SiTracks_Refitted.trackerHits_begin\"][\n",
    "        iev\n",
    "    ]  # hit_beg[i] gives the first hit index for track i\n",
    "    hit_end = event_data[\"SiTracks_Refitted/SiTracks_Refitted.trackerHits_end\"][\n",
    "        iev\n",
    "    ]  # hit_end[i] gives the last hit index for track i\n",
    "    trk_hit_idx = event_data[\"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.index\"][iev]\n",
    "    trk_hit_coll = event_data[\"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.collectionID\"][iev]\n",
    "\n",
    "    # Create a mapping from global hit indices to local (collection, index) pairs\n",
    "    _, hit_idx_local_to_global = create_global_to_local_mapping(tracker_hit_data, iev, collectionIDs)\n",
    "\n",
    "    # Initialize lists for COO matrix\n",
    "    track_to_hit_matrix_coo0 = []\n",
    "    track_to_hit_matrix_coo1 = []\n",
    "    track_to_hit_matrix_w = []\n",
    "\n",
    "    # Iterate over tracks and their associated hits\n",
    "    for track_idx, (beg, end) in enumerate(zip(hit_beg, hit_end)):\n",
    "        for ihit in range(beg, end):\n",
    "            local_hit_idx = trk_hit_idx[ihit]\n",
    "            collid = trk_hit_coll[ihit]\n",
    "\n",
    "            if (collid, local_hit_idx) not in hit_idx_local_to_global:\n",
    "                continue\n",
    "            global_hit_idx = hit_idx_local_to_global[(collid, local_hit_idx)]\n",
    "\n",
    "            # Append to COO matrix\n",
    "            track_to_hit_matrix_coo0.append(track_idx)\n",
    "            track_to_hit_matrix_coo1.append(global_hit_idx)\n",
    "            track_to_hit_matrix_w.append(1.0)  # Assuming weight is 1.0 for all associations\n",
    "\n",
    "    return (\n",
    "        (\n",
    "            np.array(track_to_hit_matrix_coo0),\n",
    "            np.array(track_to_hit_matrix_coo1),\n",
    "            np.array(track_to_hit_matrix_w),\n",
    "        ),\n",
    "        hit_idx_local_to_global,\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: check correctness of this function\n",
    "def create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs):\n",
    "    \"\"\"\n",
    "    Creates the COO matrix indices and weights for the relationship between clusters and calorimeter hits.\n",
    "\n",
    "    Args:\n",
    "        ev: The event data containing cluster property data.\n",
    "        iev: The index of the event to extract data for.\n",
    "        collectionIDs: A dictionary mapping collection names to their IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three arrays:\n",
    "            - Row indices (cluster indices).\n",
    "            - Column indices (global hit indices).\n",
    "            - Weights (association weights between clusters and hits).\n",
    "    \"\"\"\n",
    "    # Extract calorimeter hit data\n",
    "    calo_hit_data = event_data[\n",
    "        [\n",
    "            \"ECALBarrel\",\n",
    "            \"ECALEndcap\",\n",
    "            \"HCALBarrel\",\n",
    "            \"HCALEndcap\",\n",
    "            \"HCALOther\",\n",
    "            \"MUON\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Extract cluster-to-hit associations\n",
    "    cluster_hit_begin = event_data[\"PandoraClusters/PandoraClusters.hits_begin\"][iev]\n",
    "    cluster_hit_end = event_data[\"PandoraClusters/PandoraClusters.hits_end\"][iev]\n",
    "    cluster_hit_idx = event_data[\"_PandoraClusters_hits/_PandoraClusters_hits.index\"][iev]\n",
    "    cluster_hit_coll = event_data[\"_PandoraClusters_hits/_PandoraClusters_hits.collectionID\"][iev]\n",
    "\n",
    "    _, calo_hit_idx_local_to_global = create_global_to_local_mapping(calo_hit_data, iev, collectionIDs)\n",
    "\n",
    "    # Initialize lists for COO matrix\n",
    "    cluster_to_hit_matrix_coo0 = []\n",
    "    cluster_to_hit_matrix_coo1 = []\n",
    "    cluster_to_hit_matrix_w = []\n",
    "\n",
    "    # Iterate over clusters and their associated hits\n",
    "    for cluster_idx, (beg, end) in enumerate(zip(cluster_hit_begin, cluster_hit_end)):\n",
    "        for ihit in range(beg, end):\n",
    "            local_hit_idx = cluster_hit_idx[ihit]\n",
    "            collid = cluster_hit_coll[ihit]\n",
    "\n",
    "            if (collid, local_hit_idx) not in calo_hit_idx_local_to_global:\n",
    "                continue\n",
    "            global_hit_idx = calo_hit_idx_local_to_global[(collid, local_hit_idx)]\n",
    "\n",
    "            # Append to COO matrix\n",
    "            cluster_to_hit_matrix_coo0.append(cluster_idx)\n",
    "            cluster_to_hit_matrix_coo1.append(global_hit_idx)\n",
    "            cluster_to_hit_matrix_w.append(1.0)  # Assuming weight is 1.0 for all associations\n",
    "\n",
    "    return (\n",
    "        (\n",
    "            np.array(cluster_to_hit_matrix_coo0),\n",
    "            np.array(cluster_to_hit_matrix_coo1),\n",
    "            np.array(cluster_to_hit_matrix_w),\n",
    "        ),\n",
    "        calo_hit_idx_local_to_global,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_calo_hit_data(event_data, iev, collectionIDs):\n",
    "    feats = [\"type\", \"energy\", \"position.x\", \"position.y\", \"position.z\"]\n",
    "\n",
    "    calo_hit_data = event_data[\n",
    "        [\n",
    "            \"ECALBarrel\",\n",
    "            \"ECALEndcap\",\n",
    "            \"HCALBarrel\",\n",
    "            \"HCALEndcap\",\n",
    "            \"HCALOther\",\n",
    "            \"MUON\",\n",
    "        ]\n",
    "    ]\n",
    "    calohit_links = event_data[\n",
    "        [\n",
    "            \"CalohitMCTruthLink.weight\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.collectionID\",\n",
    "            \"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.index\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.collectionID\",\n",
    "            \"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.index\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Create a mapping from global hit indices to local (collection, index) pairs and hit feature matrix\n",
    "    hit_features, _, hit_idx_local_to_global = create_hit_feature_matrix_and_mapping(\n",
    "        calo_hit_data, iev, collectionIDs, feats\n",
    "    )\n",
    "\n",
    "    # Add all edges from genparticle to calohit\n",
    "    calohit_to_gen_weight = calohit_links[\"CalohitMCTruthLink.weight\"][iev]\n",
    "    calohit_to_gen_calo_colid = calohit_links[\"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.collectionID\"][iev]\n",
    "    calohit_to_gen_gen_colid = calohit_links[\"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.collectionID\"][iev]\n",
    "    calohit_to_gen_calo_idx = calohit_links[\"_CalohitMCTruthLink_from/_CalohitMCTruthLink_from.index\"][iev]\n",
    "    calohit_to_gen_gen_idx = calohit_links[\"_CalohitMCTruthLink_to/_CalohitMCTruthLink_to.index\"][iev]\n",
    "\n",
    "    genparticle_to_hit_matrix_coo0 = []\n",
    "    genparticle_to_hit_matrix_coo1 = []\n",
    "    genparticle_to_hit_matrix_w = []\n",
    "    for calo_colid, calo_idx, gen_colid, gen_idx, weight in zip(\n",
    "        calohit_to_gen_calo_colid,\n",
    "        calohit_to_gen_calo_idx,\n",
    "        calohit_to_gen_gen_colid,\n",
    "        calohit_to_gen_gen_idx,\n",
    "        calohit_to_gen_weight,\n",
    "    ):\n",
    "        genparticle_to_hit_matrix_coo0.append(gen_idx)\n",
    "        genparticle_to_hit_matrix_coo1.append(hit_idx_local_to_global[(calo_colid, calo_idx)])\n",
    "        genparticle_to_hit_matrix_w.append(weight)\n",
    "\n",
    "    return (\n",
    "        hit_features,\n",
    "        (\n",
    "            np.array(genparticle_to_hit_matrix_coo0),\n",
    "            np.array(genparticle_to_hit_matrix_coo1),\n",
    "            np.array(genparticle_to_hit_matrix_w),\n",
    "        ),\n",
    "        hit_idx_local_to_global,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_tracker_hit_data(event_data, iev, collectionIDs):\n",
    "\n",
    "    feats = [\"type\", \"energy\", \"position.x\", \"position.y\", \"position.z\"]\n",
    "\n",
    "    tracker_hit_data = event_data[\n",
    "        [\n",
    "            \"VXDTrackerHits\",\n",
    "            \"VXDEndcapTrackerHits\",\n",
    "            \"ITrackerHits\",\n",
    "            \"OTrackerHits\",\n",
    "            # \"ITrackerEndcapHits\",\n",
    "            # \"OTrackerEndcapHits\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Create a mapping from global hit indices to local (collection, index) pairs and hit feature matrix\n",
    "    hit_feature_matrix, _, hit_idx_local_to_global = create_hit_feature_matrix_and_mapping(\n",
    "        tracker_hit_data, iev, collectionIDs, feats\n",
    "    )\n",
    "\n",
    "    # Extract tracker hit to track associations\n",
    "    hit_beg = event_data[\"SiTracks_Refitted/SiTracks_Refitted.trackerHits_begin\"][\n",
    "        iev\n",
    "    ]  # hit_beg[i] gives the first hit index for track i\n",
    "    hit_end = event_data[\"SiTracks_Refitted/SiTracks_Refitted.trackerHits_end\"][\n",
    "        iev\n",
    "    ]  # hit_end[i] gives the last hit index for track i\n",
    "    trk_hit_idx = event_data[\"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.index\"][iev]\n",
    "    trk_hit_coll = event_data[\"_SiTracks_Refitted_trackerHits/_SiTracks_Refitted_trackerHits.collectionID\"][iev]\n",
    "\n",
    "    # Get the COO format adjacency data\n",
    "    genparticle_to_track_matrix_coo0, genparticle_to_track_matrix_coo1, genparticle_to_track_matrix_w = (\n",
    "        genparticle_track_adj(event_data, iev)\n",
    "    )\n",
    "    gen_indices = genparticle_to_track_matrix_coo0\n",
    "    track_indices = genparticle_to_track_matrix_coo1\n",
    "\n",
    "    # Initialize lists for COO matrix\n",
    "    genparticle_to_hit_matrix_coo0 = []\n",
    "    genparticle_to_hit_matrix_coo1 = []\n",
    "    genparticle_to_hit_matrix_w = []\n",
    "\n",
    "    # Iterate over non-zero elements to find links between genparticles and tracks\n",
    "    for gen_idx, track_idx, weight in zip(gen_indices, track_indices, genparticle_to_track_matrix_w):\n",
    "        if weight > 0:  # Only consider non-zero weights\n",
    "            # Find tracker hits associated with the track\n",
    "            for ihit in range(hit_beg[track_idx], hit_end[track_idx]):  # for all hits in this track\n",
    "                # Translate local hit index to global hit index\n",
    "                local_hit_idx = trk_hit_idx[ihit]\n",
    "                collid = trk_hit_coll[ihit]\n",
    "\n",
    "                if (\n",
    "                    collid,\n",
    "                    local_hit_idx,\n",
    "                ) not in hit_idx_local_to_global:  # Check if the hit is in the local-to-global mapping\n",
    "                    continue\n",
    "                global_hit_idx = hit_idx_local_to_global[(collid, local_hit_idx)]\n",
    "\n",
    "                # Append the gp to hit association and weight to the COO matrix\n",
    "                genparticle_to_hit_matrix_coo0.append(gen_idx)\n",
    "                genparticle_to_hit_matrix_coo1.append(global_hit_idx)\n",
    "                genparticle_to_hit_matrix_w.append(weight)\n",
    "\n",
    "    return (\n",
    "        hit_feature_matrix,  # Tracker hit feature matrix\n",
    "        (\n",
    "            np.array(genparticle_to_hit_matrix_coo0),\n",
    "            np.array(genparticle_to_hit_matrix_coo1),\n",
    "            np.array(genparticle_to_hit_matrix_w),\n",
    "        ),\n",
    "        hit_idx_local_to_global,\n",
    "    )\n",
    "\n",
    "\n",
    "def gen_to_features(event_data, iev):\n",
    "\n",
    "    gen_data = event_data[\"MCParticles\"]\n",
    "\n",
    "    mc_coll = \"MCParticles\"\n",
    "    gen_arr = gen_data[iev]\n",
    "\n",
    "    gen_arr = {k.replace(mc_coll + \".\", \"\"): gen_arr[k] for k in gen_arr.fields}\n",
    "\n",
    "    MCParticles_p4 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"mass\": gen_arr[\"mass\"],\n",
    "                \"x\": gen_arr[\"momentum.x\"],\n",
    "                \"y\": gen_arr[\"momentum.y\"],\n",
    "                \"z\": gen_arr[\"momentum.z\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    gen_arr[\"pt\"] = MCParticles_p4.pt\n",
    "    gen_arr[\"eta\"] = MCParticles_p4.eta\n",
    "    gen_arr[\"phi\"] = MCParticles_p4.phi\n",
    "    gen_arr[\"energy\"] = MCParticles_p4.energy\n",
    "    gen_arr[\"sin_phi\"] = np.sin(gen_arr[\"phi\"])\n",
    "    gen_arr[\"cos_phi\"] = np.cos(gen_arr[\"phi\"])\n",
    "\n",
    "    ret = {\n",
    "        \"PDG\": gen_arr[\"PDG\"],\n",
    "        \"generatorStatus\": gen_arr[\"generatorStatus\"],\n",
    "        \"charge\": gen_arr[\"charge\"],\n",
    "        \"pt\": gen_arr[\"pt\"],\n",
    "        \"eta\": gen_arr[\"eta\"],\n",
    "        \"phi\": gen_arr[\"phi\"],\n",
    "        \"sin_phi\": gen_arr[\"sin_phi\"],\n",
    "        \"cos_phi\": gen_arr[\"cos_phi\"],\n",
    "        \"energy\": gen_arr[\"energy\"],\n",
    "        # \"ispu\": gen_arr[\"ispu\"],\n",
    "        \"simulatorStatus\": gen_arr[\"simulatorStatus\"],\n",
    "        # \"gp_to_track\": np.zeros(len(gen_arr[\"PDG\"]), dtype=np.float64),\n",
    "        # \"gp_to_cluster\": np.zeros(len(gen_arr[\"PDG\"]), dtype=np.float64),\n",
    "        # \"jet_idx\": np.zeros(len(gen_arr[\"PDG\"]), dtype=np.int64),\n",
    "        # \"daughters_begin\": gen_arr[\"daughters_begin\"],\n",
    "        # \"daughters_end\": gen_arr[\"daughters_end\"],\n",
    "        \"px\": gen_arr[\"momentum.x\"],\n",
    "        \"py\": gen_arr[\"momentum.y\"],\n",
    "        \"pz\": gen_arr[\"momentum.z\"],\n",
    "        \"mass\": gen_arr[\"mass\"],\n",
    "    }\n",
    "\n",
    "    # ret[\"index\"] = prop_data[\"_MCParticles_daughters/_MCParticles_daughters.index\"][iev]\n",
    "\n",
    "    # make all values numpy arrays\n",
    "    ret = {k: awkward.to_numpy(v) for k, v in ret.items()}\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "# From https://bib-pubdb1.desy.de/record/81214/files/LC-DET-2006-004[1].pdf, eq12\n",
    "# pTâ€‹(in GeV/c) â‰ˆ a [mm/s] * |Bz(in T) / omega(1/mm)|\n",
    "# a = c * 10^(-15) = 3*10^(-4)\n",
    "def track_pt(omega, bfield=B):\n",
    "    a = 3 * 10**-4\n",
    "    return a * np.abs(bfield / omega)\n",
    "\n",
    "\n",
    "def track_to_features(event_data, iev):\n",
    "    \"\"\"\n",
    "    Extracts track features from the provided property data for a specific event and track collection.\n",
    "\n",
    "    Args:\n",
    "        event_data (awdward.Array) The event data containing track property data.\n",
    "        iev (int): The index of the event to extract data for.\n",
    "\n",
    "    Returns:\n",
    "        awkward.Record: A record containing the extracted track features, including:\n",
    "            - \"type\", \"chi2\", \"ndf\": Basic track properties.\n",
    "            - \"dEdx\", \"dEdxError\": Energy deposition and its error.\n",
    "            - \"radiusOfInnermostHit\": Radius of the innermost hit for each track.\n",
    "            - \"tanLambda\", \"D0\", \"phi\", \"omega\", \"Z0\", \"time\": Track state properties.\n",
    "            - \"pt\", \"px\", \"py\", \"pz\", \"p\": Momentum components and magnitude.\n",
    "            - \"eta\": Pseudorapidity.\n",
    "            - \"sin_phi\", \"cos_phi\": Sine and cosine of the azimuthal angle.\n",
    "            - \"elemtype\": Element type (always 1 for tracks).\n",
    "            - \"q\": Charge of the track (+1 or -1).\n",
    "\n",
    "    Notes:\n",
    "        - The function calculates additional derived features such as momentum components, pseudorapidity,\n",
    "          and radius of the innermost hit.\n",
    "        - The \"AtFirstHit\" state is used to determine the innermost hit radius.\n",
    "        - The charge is set to +1 or -1 based on the sign of the \"omega\" parameter.\n",
    "        - The input `ev` is expected to be an uproot TTree object containing the necessary branches.\n",
    "    \"\"\"\n",
    "    track_coll = \"SiTracks_Refitted\"\n",
    "    track_arr = event_data[track_coll][iev]\n",
    "    track_arr_dQdx = event_data[\"SiTracks_Refitted_dQdx\"][iev]\n",
    "    track_arr_trackStates = event_data[\"_SiTracks_Refitted_trackStates\"][iev]\n",
    "\n",
    "    feats_from_track = [\"type\", \"chi2\", \"ndf\"]\n",
    "    ret = {feat: track_arr[track_coll + \".\" + feat] for feat in feats_from_track}\n",
    "\n",
    "    ret[\"dEdx\"] = track_arr_dQdx[\"SiTracks_Refitted_dQdx.dQdx.value\"]\n",
    "    ret[\"dEdxError\"] = track_arr_dQdx[\"SiTracks_Refitted_dQdx.dQdx.error\"]\n",
    "\n",
    "    # build the radiusOfInnermostHit variable\n",
    "    num_tracks = len(ret[\"dEdx\"])\n",
    "    innermost_radius = []\n",
    "    for itrack in range(num_tracks):\n",
    "\n",
    "        # select the track states corresponding to itrack\n",
    "        # pick the state AtFirstHit\n",
    "        # https://github.com/key4hep/EDM4hep/blob/fe5a54046a91a7e648d0b588960db7841aebc670/edm4hep.yaml#L220\n",
    "        ibegin = track_arr[track_coll + \".\" + \"trackStates_begin\"][itrack]\n",
    "        iend = track_arr[track_coll + \".\" + \"trackStates_end\"][itrack]\n",
    "\n",
    "        refX = track_arr_trackStates[\"_SiTracks_Refitted_trackStates\" + \".\" + \"referencePoint.x\"][ibegin:iend]\n",
    "        refY = track_arr_trackStates[\"_SiTracks_Refitted_trackStates\" + \".\" + \"referencePoint.y\"][ibegin:iend]\n",
    "        location = track_arr_trackStates[\"_SiTracks_Refitted_trackStates\" + \".\" + \"location\"][ibegin:iend]\n",
    "\n",
    "        istate = np.argmax(location == 2)  # 2 refers to AtFirstHit\n",
    "\n",
    "        innermost_radius.append(math.sqrt(refX[istate] ** 2 + refY[istate] ** 2))\n",
    "\n",
    "    ret[\"radiusOfInnermostHit\"] = np.array(innermost_radius)\n",
    "\n",
    "    # get the index of the first track state\n",
    "    trackstate_idx = event_data[track_coll][track_coll + \".trackStates_begin\"][iev]\n",
    "    # get the properties of the track at the first track state (at the origin)\n",
    "    for k in [\"tanLambda\", \"D0\", \"phi\", \"omega\", \"Z0\", \"time\"]:\n",
    "        ret[k] = awkward.to_numpy(\n",
    "            event_data[\"_SiTracks_Refitted_trackStates\"][\"_SiTracks_Refitted_trackStates.\" + k][iev][trackstate_idx]\n",
    "        )\n",
    "\n",
    "    ret[\"pt\"] = awkward.to_numpy(track_pt(ret[\"omega\"]))\n",
    "    ret[\"px\"] = awkward.to_numpy(np.cos(ret[\"phi\"])) * ret[\"pt\"]\n",
    "    ret[\"py\"] = awkward.to_numpy(np.sin(ret[\"phi\"])) * ret[\"pt\"]\n",
    "    ret[\"pz\"] = ret[\"pt\"] * ret[\"tanLambda\"]\n",
    "    ret[\"p\"] = np.sqrt(ret[\"px\"] ** 2 + ret[\"py\"] ** 2 + ret[\"pz\"] ** 2)\n",
    "    cos_theta = np.divide(ret[\"pz\"], ret[\"p\"], where=ret[\"p\"] > 0)\n",
    "    theta = np.arccos(cos_theta)\n",
    "    tt = np.tan(theta / 2.0)\n",
    "    eta = awkward.to_numpy(-np.log(tt, where=tt > 0))\n",
    "    eta[tt <= 0] = 0.0\n",
    "    ret[\"eta\"] = eta\n",
    "\n",
    "    ret[\"sin_phi\"] = np.sin(ret[\"phi\"])\n",
    "    ret[\"cos_phi\"] = np.cos(ret[\"phi\"])\n",
    "\n",
    "    # track is always type 1\n",
    "    ret[\"elemtype\"] = 1 * np.ones(num_tracks, dtype=np.float32)\n",
    "\n",
    "    ret[\"q\"] = ret[\"omega\"].copy()\n",
    "    ret[\"q\"][ret[\"q\"] > 0] = 1\n",
    "    ret[\"q\"][ret[\"q\"] < 0] = -1\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def cluster_to_features(event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\"]):\n",
    "    \"\"\"\n",
    "    Extracts cluster features for a specific event.\n",
    "\n",
    "    Args:\n",
    "        ev: The event data containing cluster property data.\n",
    "        iev: The index of the event to extract data for.\n",
    "        cluster_features (list of str): A list of cluster feature names to extract.\n",
    "            Default is [\"position.x\", \"position.y\", \"position.z\", \"energy\"].\n",
    "    Returns:\n",
    "        dict: A dictionary containing cluster features for the specified event.\n",
    "    Raises:\n",
    "        ValueError: If a specified feature is not found in the event data.\n",
    "    \"\"\"\n",
    "    cluster_data = event_data[\"PandoraClusters\"]\n",
    "    for feat in cluster_features:\n",
    "        if f\"PandoraClusters.{feat}\" not in cluster_data.fields:\n",
    "            raise ValueError(f\"Feature {feat} not found in PandoraClusters.\")\n",
    "        # Extract cluster features\n",
    "\n",
    "    return {f\"{feat}\": awkward.to_numpy(cluster_data[f\"PandoraClusters.{feat}\"][iev]) for feat in cluster_features}\n",
    "\n",
    "\n",
    "def create_genparticle_to_genparticle_coo_matrix(event_data, iev):\n",
    "    \"\"\"\n",
    "    Creates the COO matrix indices and weights for the relationship between genparticles\n",
    "    based on parent-daughter associations.\n",
    "\n",
    "    Args:\n",
    "        ev: The event data containing genparticle property data.\n",
    "        iev: The index of the event to extract data for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three arrays:\n",
    "            - Row indices (parent genparticle indices).\n",
    "            - Column indices (daughter genparticle indices).\n",
    "            - Weights (association weights between parent and daughter genparticles, set to 1.0).\n",
    "    \"\"\"\n",
    "    # Extract parent-daughter associations\n",
    "    daughters_begin = event_data[\"MCParticles.daughters_begin\"][iev]\n",
    "    daughters_end = event_data[\"MCParticles.daughters_end\"][iev]\n",
    "    daughter_indices = event_data[\"_MCParticles_daughters/_MCParticles_daughters.index\"][iev]\n",
    "\n",
    "    # Initialize lists for COO matrix\n",
    "    coo_rows = []\n",
    "    coo_cols = []\n",
    "    coo_weights = []\n",
    "\n",
    "    # Iterate over genparticles and their associated daughters\n",
    "    for parent_idx, (beg, end) in enumerate(zip(daughters_begin, daughters_end)):\n",
    "        for idaughter in range(beg, end):\n",
    "            daughter_idx = daughter_indices[idaughter]\n",
    "            coo_rows.append(parent_idx)\n",
    "            coo_cols.append(daughter_idx)\n",
    "            coo_weights.append(1.0)  # Assuming weight is 1.0 for all associations\n",
    "\n",
    "    return (\n",
    "        np.array(coo_rows),\n",
    "        np.array(coo_cols),\n",
    "        np.array(coo_weights),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_genparticle_to_genparticle_coo_matrix2(event_data, iev):\n",
    "    \"\"\"\n",
    "    Creates the COO matrix indices and weights for the relationship between genparticles\n",
    "    based on parent-daughter associations.\n",
    "\n",
    "    Args:\n",
    "        ev: The event data containing genparticle property data.\n",
    "        iev: The index of the event to extract data for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three arrays:\n",
    "            - Row indices (parent genparticle indices).\n",
    "            - Column indices (daughter genparticle indices).\n",
    "            - Weights (association weights between parent and daughter genparticles, set to 1.0).\n",
    "    \"\"\"\n",
    "    parents_begin = event_data[\"MCParticles.parents_begin\"][iev]\n",
    "    parents_end = event_data[\"MCParticles.parents_end\"][iev]\n",
    "    parent_indices = event_data[\"_MCParticles_parents/_MCParticles_parents.index\"][iev]\n",
    "\n",
    "    # Initialize lists for COO matrix\n",
    "    coo_rows = []\n",
    "    coo_cols = []\n",
    "    coo_weights = []\n",
    "\n",
    "    # Iterate over genparticles and their associated parents\n",
    "    for daughter_idx, (beg, end) in enumerate(zip(parents_begin, parents_end)):\n",
    "        for iparent in range(beg, end):\n",
    "            parent_idx = parent_indices[iparent]\n",
    "            coo_rows.append(parent_idx)\n",
    "            coo_cols.append(daughter_idx)\n",
    "            coo_weights.append(1.0)  # Assuming weight is 1.0 for all associations\n",
    "\n",
    "    return (\n",
    "        np.array(coo_rows),\n",
    "        np.array(coo_cols),\n",
    "        np.array(coo_weights),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to extract:\n",
    "- [x] hits (all tracker and calo hit features in the event)\n",
    "- [x] genparticles (all genparticle features in the event, noting that the genparticles are in the form of a decay tree)\n",
    "- [x] genparticle_to_genparticle (sparse association matrix for the genparticle decay tree)\n",
    "- [x] hits_to_genparticles (sparse association matrix)\n",
    "- [x] tracks (all track features in the event)\n",
    "- [x] tracks_to_hits (sparse association matrix between tracks and tracker hits)\n",
    "- [x] clusters (all cluster features in the event)\n",
    "- [x] clusters_to_hits (sparse association matrix between clusters and calo hits)\n",
    "\n",
    "Additional data to extract:\n",
    "- [x] genparticle_to_track (sparse association matrix for the genparticle decay tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files_dir = Path(\"/mnt/ceph/users/ewulff/data/cld/Dec3/subfolder_0/\")\n",
    "root_file = root_files_dir / \"reco_p8_ee_tt_ecm365_10000.root\"\n",
    "fi = uproot.open(root_file)\n",
    "ev = fi[\"events\"]\n",
    "\n",
    "# which event to pick from the file\n",
    "iev = 2\n",
    "\n",
    "collectionIDs = {\n",
    "    k: v\n",
    "    for k, v in zip(\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\"events___idTable/m_collectionIDs\"][0],\n",
    "    )\n",
    "}\n",
    "\n",
    "event_data = get_event_data(ev)\n",
    "\n",
    "# Extract gparticle, track, and cluster features\n",
    "gen_features = gen_to_features(event_data, iev)\n",
    "track_features = track_to_features(event_data, iev)\n",
    "cluster_features = cluster_to_features(\n",
    "    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    ")\n",
    "\n",
    "# Process calorimeter hit data\n",
    "calo_hit_features, genparticle_to_calo_hit_matrix, calo_hit_idx_local_to_global = process_calo_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Process tracker hit data\n",
    "tracker_hit_features, genparticle_to_tracker_hit_matrix, tracker_hit_idx_local_to_global = process_tracker_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Create the track-to-trackerhit adjacency matrix\n",
    "track_to_tracker_hit_matrix, tracker_hit_idx_local_to_global_2 = create_track_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    tracker_hit_idx_local_to_global == tracker_hit_idx_local_to_global_2\n",
    "), \"Local to global tracker hit index mapping mismatch!\"\n",
    "\n",
    "# Ceate the cluster-to-clusterhit adjacency matrix\n",
    "cluster_to_cluster_hit_matrix, calo_hit_idx_local_to_global_2 = create_cluster_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    calo_hit_idx_local_to_global == calo_hit_idx_local_to_global_2\n",
    "), \"Local to global calorimeter hit index mapping mismatch!\"\n",
    "\n",
    "# Create the genparticle-to-track adjacency matrix\n",
    "gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "\n",
    "# Create the genparticle-to-genparticle adjacency matrix\n",
    "gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "# Check consistency between the two methods gp-to-gp methods\n",
    "gp_to_gp2 = create_genparticle_to_genparticle_coo_matrix2(event_data, iev)\n",
    "\n",
    "# Get the number of genparticles in the event\n",
    "n_gp = len(ev[\"MCParticles.momentum.x\"].array()[iev])\n",
    "\n",
    "# create coo matrix through the COOs in gp_to_gp and gp_to_gp2\n",
    "coo_matrix_gp_to_gp = coo_matrix((gp_to_gp[2], (gp_to_gp[0], gp_to_gp[1])), shape=(n_gp, n_gp))\n",
    "coo_matrix_gp_to_gp2 = coo_matrix((gp_to_gp2[2], (gp_to_gp2[0], gp_to_gp2[1])), shape=(n_gp, n_gp))\n",
    "# Check if the two dense gp_to_gp matrices are equal\n",
    "assert (coo_matrix_gp_to_gp.todense() == coo_matrix_gp_to_gp2.todense()).all()\n",
    "# Define the output file path\n",
    "output_file = Path(\"extracted_features.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features), type(track_features), type(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features[\"px\"]), type(track_features[\"px\"]), type(cluster_features[\"position.x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features), type(tracker_hit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features[\"energy\"]), type(tracker_hit_features[\"energy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix),\n",
    "    type(genparticle_to_tracker_hit_matrix),\n",
    "    type(track_to_tracker_hit_matrix),\n",
    "    type(cluster_to_cluster_hit_matrix),\n",
    "    type(gp_to_gp),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix[0]),\n",
    "    type(genparticle_to_tracker_hit_matrix[0]),\n",
    "    type(track_to_tracker_hit_matrix[0]),\n",
    "    type(cluster_to_cluster_hit_matrix[0]),\n",
    "    type(gp_to_gp[0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix[0][0]),\n",
    "    type(genparticle_to_tracker_hit_matrix[0][0]),\n",
    "    type(track_to_tracker_hit_matrix[0][0]),\n",
    "    type(cluster_to_cluster_hit_matrix[0][0]),\n",
    "    type(gp_to_gp[0][0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from calo_hit_features and tracker_hit_features into a single dataframe\n",
    "df = pandas.DataFrame()\n",
    "\n",
    "# Extract data from calo_hit_features and tracker_hit_features using numpy.concatenate\n",
    "df[\"px\"] = np.concatenate([calo_hit_features[\"position.x\"], tracker_hit_features[\"position.x\"]])\n",
    "df[\"py\"] = np.concatenate([calo_hit_features[\"position.y\"], tracker_hit_features[\"position.y\"]])\n",
    "df[\"pz\"] = np.concatenate([calo_hit_features[\"position.z\"], tracker_hit_features[\"position.z\"]])\n",
    "df[\"energy\"] = np.concatenate([1000 * calo_hit_features[\"energy\"], 1000 * tracker_hit_features[\"energy\"]])\n",
    "df[\"plotsize\"] = 0.0\n",
    "df[\"subdetector\"] = np.concatenate([calo_hit_features[\"subdetector\"], tracker_hit_features[\"subdetector\"]])\n",
    "\n",
    "# Calculate plotsize based on subdetector values\n",
    "df.loc[df[\"subdetector\"] == 0, \"plotsize\"] = df.loc[df[\"subdetector\"] == 0, \"energy\"] / 5.0\n",
    "df.loc[df[\"subdetector\"] == 1, \"plotsize\"] = df.loc[df[\"subdetector\"] == 1, \"energy\"] / 10.0\n",
    "df.loc[df[\"subdetector\"] == 2, \"plotsize\"] = df.loc[df[\"subdetector\"] == 2, \"energy\"] * 100.0\n",
    "df.loc[df[\"subdetector\"] == 3, \"plotsize\"] = df.loc[df[\"subdetector\"] == 3, \"energy\"] * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks\n",
    "def helix_eq(charge, bfield, v):\n",
    "    \"\"\"Calculate the 3D helical trajectory of a charged particle in a magnetic field.\n",
    "\n",
    "    This function computes the x, y, and z coordinates of a charged particle's\n",
    "    helical trajectory in a uniform magnetic field over a range of time values.\n",
    "\n",
    "    Args:\n",
    "        charge (float): The charge of the particle in elementary charge units.\n",
    "        bfield (float): The magnetic field strength in Tesla.\n",
    "        v (vector): A vector object representing the particle's velocity and properties.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of three lists (x, y, z) representing the particle's\n",
    "        trajectory in 3D space:\n",
    "            - x (list): x-coordinates of the trajectory.\n",
    "            - y (list): y-coordinates of the trajectory.\n",
    "            - z (list): z-coordinates of the trajectory.\n",
    "    \"\"\"\n",
    "    R = v.pt / (charge * 0.3 * bfield)\n",
    "    omega = charge * 0.3 * bfield / (v.gamma * v.mass)\n",
    "    t_values = np.linspace(0, 2 / (c * v.beta), 10)\n",
    "    x = list(scale * R * np.cos(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.cos(v.phi - np.pi / 2))\n",
    "    y = list(scale * R * np.sin(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.sin(v.phi - np.pi / 2))\n",
    "    z = list(scale * v.pz * c * t_values / (v.gamma * v.mass))\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "track_px, track_py, track_pz, track_charge = (\n",
    "    track_features[\"px\"],\n",
    "    track_features[\"py\"],\n",
    "    track_features[\"pz\"],\n",
    "    track_features[\"q\"],\n",
    ")\n",
    "\n",
    "track_mass = np.zeros_like(track_px) + pion_mass\n",
    "\n",
    "track_x, track_y, track_z = [], [], []\n",
    "for irow in range(len(track_px)):\n",
    "\n",
    "    # convert to vector\n",
    "    v = vector.obj(px=track_px[irow], py=track_py[irow], pz=track_pz[irow], mass=track_mass[irow])\n",
    "\n",
    "    x, y, z = helix_eq(track_charge[irow], B, v)\n",
    "    track_x += x\n",
    "    track_y += y\n",
    "    track_z += z\n",
    "\n",
    "    track_x += [None]  # Add None to separate tracks in the plot\n",
    "    track_y += [None]\n",
    "    track_z += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"Raw ECAL hit\", 1: \"Raw HCAL hit\", 2: \"Raw Muon chamber hit\", 3: \"Raw tracker hit\"}\n",
    "\n",
    "subdetector_color = {0: \"steelblue\", 1: \"green\", 2: \"orange\", 3: \"red\"}  # ECAL  # HCAL  # MUON  # Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "# raw hits\n",
    "for subdetector in [0, 1, 2, 3]:\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=np.clip(df[\"px\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        y=np.clip(df[\"py\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        z=np.clip(df[\"pz\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=np.clip(2 + 2 * np.log(df[\"plotsize\"]), 1, 15),\n",
    "            color=subdetector_color[subdetector],\n",
    "            colorscale=\"Viridis\",\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name=labels[subdetector],\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# add the tracks\n",
    "trace = go.Scatter3d(\n",
    "    x=np.array(track_x),\n",
    "    y=np.array(track_y),\n",
    "    z=np.array(track_z),\n",
    "    mode=\"lines\",\n",
    "    name=\"Track\",\n",
    "    line=dict(color=\"red\"),\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# # add the clusters\n",
    "trace = go.Scatter3d(\n",
    "    x=cluster_features[\"position.x\"],\n",
    "    y=cluster_features[\"position.y\"],\n",
    "    z=cluster_features[\"position.z\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=cluster_features[\"energy\"],\n",
    "        color=\"blue\",\n",
    "        opacity=0.8,\n",
    "    ),\n",
    "    name=\"ECAL/HCAL clusters\",\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    legend=dict(x=0.8, y=0.5, font=dict(size=16)),  # https://plotly.com/python/legend/\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # for plotly to avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_tracks_rawcalohits.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg_dict = {\n",
    "    22: \"photon\",  # photon\n",
    "    11: \"electron\",  # electron\n",
    "    13: \"muon\",  # muon\n",
    "    130: \"n. hadron\",  # neutral hadron\n",
    "    211: \"ch. hadron\",  # charged hadron\n",
    "}\n",
    "\n",
    "color_dict = {\n",
    "    \"photon\": \"red\",  # photon\n",
    "    \"electron\": \"green\",  # electron\n",
    "    \"muon\": \"purple\",  # muon\n",
    "    \"n. hadron\": \"orange\",  # neutral hadron\n",
    "    \"ch. hadron\": \"blue\",  # charged hadron\n",
    "    None: \"black\",  # placeholder for skipped element\n",
    "}\n",
    "\n",
    "# Extract relevant data from gen_features\n",
    "gen_px = gen_features[\"px\"]\n",
    "gen_py = gen_features[\"py\"]\n",
    "gen_pz = gen_features[\"pz\"]\n",
    "gen_mass = gen_features[\"mass\"]\n",
    "gen_charge = gen_features[\"charge\"]\n",
    "gen_pdg = awkward.to_numpy(np.absolute(gen_features[\"PDG\"]))\n",
    "\n",
    "# Set all other particles to ch.had or n.had\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & awkward.to_numpy(np.abs(gen_charge) > 0)] = (\n",
    "    211  # when not filtering genstatus==1, charge can be between 0 and 1\n",
    ")\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & awkward.to_numpy(np.abs(gen_charge) == 0)] = 130\n",
    "\n",
    "# Extrapolate MC particle trajectories\n",
    "mc_x = []\n",
    "mc_y = []\n",
    "mc_z = []\n",
    "pdg_list = []\n",
    "for irow in range(len(gen_px)):\n",
    "\n",
    "    # Convert to vector\n",
    "    v = vector.obj(px=gen_px[irow], py=gen_py[irow], pz=gen_pz[irow], mass=gen_mass[irow])\n",
    "    if gen_charge[irow] == 0:\n",
    "        this_mc_x = [0, np.clip(scale * v.px / v.mag, -4000, 4000)]\n",
    "        this_mc_y = [0, np.clip(scale * v.py / v.mag, -4000, 4000)]\n",
    "        this_mc_z = [0, np.clip(scale * v.pz / v.mag, -4000, 4000)]\n",
    "    else:\n",
    "        x, y, z = helix_eq(gen_charge[irow], B, v)\n",
    "        this_mc_x = x\n",
    "        this_mc_y = y\n",
    "        this_mc_z = z\n",
    "\n",
    "    pdg_list += len(this_mc_x) * [pdg_dict[gen_pdg[irow]]]\n",
    "\n",
    "    mc_x += this_mc_x\n",
    "    mc_y += this_mc_y\n",
    "    mc_z += this_mc_z\n",
    "\n",
    "    mc_x += [None]\n",
    "    mc_y += [None]\n",
    "    mc_z += [None]\n",
    "    pdg_list += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D scatter plot with one trace per particle\n",
    "traces = []\n",
    "unique_particles = set(pdg_list)  # Get unique particle types\n",
    "\n",
    "for particle in unique_particles:\n",
    "\n",
    "    # Get indices for the current particle, including None at the end of each track\n",
    "    indices = []\n",
    "    for i, p in enumerate(pdg_list):\n",
    "        if p is None:  # we don't need to add non-particles\n",
    "            continue\n",
    "        if p == particle:\n",
    "            indices.append(i)\n",
    "            if pdg_list[i + 1] is None:\n",
    "                indices.append(i + 1)  # Add None at the end of the track to separate tracks in plot\n",
    "\n",
    "    # Create a separate trace for each particle\n",
    "    traces.append(\n",
    "        go.Scatter3d(\n",
    "            x=np.array(mc_x)[indices],\n",
    "            y=np.array(mc_y)[indices],\n",
    "            z=np.array(mc_z)[indices],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color_dict[particle]),  # Assign color for the particle\n",
    "            name=f\"{particle}\",  # Add particle name to the legend\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # Avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_particles_legend.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the alignment of extrapolated tracks and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for itrk in range(9):\n",
    "    plt.sca(axes[itrk])\n",
    "    v = vector.obj(px=track_px[itrk], py=track_py[itrk], pz=track_pz[itrk], mass=track_mass[itrk])\n",
    "\n",
    "    # Get the global hit indices associated with the current track\n",
    "    # track_to_tracker_hit_matrix is a tuple where the first element contains the track indices and the second element contains the hit indices\n",
    "    hit_indices = track_to_tracker_hit_matrix[1][track_to_tracker_hit_matrix[0] == itrk]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = tracker_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_z = tracker_hit_features[\"position.z\"][hit_indices]\n",
    "\n",
    "    # Calculate the helix trajectory\n",
    "    x, y, z = helix_eq(track_charge[itrk], B, v)\n",
    "\n",
    "    # Plot the track and associated hits\n",
    "    plt.plot(x, z, label=\"Track\")\n",
    "    plt.scatter(hs_x, hs_z, color=\"red\", marker=\".\", label=\"Tracker hit\")\n",
    "    plt.xlim(-2000, 2000)\n",
    "    plt.ylim(-2000, 2000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[itrk].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the alignment of clusters and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for icls in range(9):\n",
    "    plt.sca(axes[icls])\n",
    "\n",
    "    # Get the global hit indices associated with the current cluster\n",
    "    # cluster_to_cluster_hit_matrix is a tuple where the first element contains the cluster indices and the second element contains the hit indices\n",
    "    hit_indices = cluster_to_cluster_hit_matrix[1][cluster_to_cluster_hit_matrix[0] == icls]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = calo_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_y = calo_hit_features[\"position.y\"][hit_indices]\n",
    "\n",
    "    # Plot the cluster and associated hits\n",
    "    plt.scatter(\n",
    "        cluster_features[\"position.x\"][icls],\n",
    "        cluster_features[\"position.y\"][icls],\n",
    "        s=100 * cluster_features[\"energy\"][icls],\n",
    "        alpha=0.5,\n",
    "        label=\"cluster\",\n",
    "    )\n",
    "    plt.scatter(hs_x, hs_y, color=\"red\", marker=\".\", label=\"hit\")\n",
    "    plt.xlim(-4000, 4000)\n",
    "    plt.ylim(-4000, 4000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[icls].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_event_to_hdf5(\n",
    "    output_file,\n",
    "    iev,\n",
    "    gen_features,\n",
    "    track_features,\n",
    "    cluster_features,\n",
    "    calo_hit_features,\n",
    "    tracker_hit_features,\n",
    "    genparticle_to_calo_hit_matrix,\n",
    "    genparticle_to_tracker_hit_matrix,\n",
    "    track_to_tracker_hit_matrix,\n",
    "    cluster_to_cluster_hit_matrix,\n",
    "    gp_to_track_matrix,\n",
    "    gp_to_gp,\n",
    "):\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        # Create a group for the event\n",
    "        event_group = h5f.create_group(f\"event_{iev}\")\n",
    "\n",
    "        # Save genparticle features\n",
    "        for key, value in gen_features.items():\n",
    "            event_group.create_dataset(f\"gen_features/{key}\", data=value)\n",
    "\n",
    "        # Save track features\n",
    "        for key, value in track_features.items():\n",
    "            event_group.create_dataset(f\"track_features/{key}\", data=value)\n",
    "\n",
    "        # Save cluster features\n",
    "        for key, value in cluster_features.items():\n",
    "            event_group.create_dataset(f\"cluster_features/{key}\", data=value)\n",
    "\n",
    "        # Save calorimeter hit features\n",
    "        for key, value in calo_hit_features.items():\n",
    "            event_group.create_dataset(f\"calo_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "        # Save tracker hit features\n",
    "        for key, value in tracker_hit_features.items():\n",
    "            event_group.create_dataset(f\"tracker_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "        # Save adjacency matrices\n",
    "        event_group.create_dataset(\"genparticle_to_calo_hit_matrix/rows\", data=genparticle_to_calo_hit_matrix[0])\n",
    "        event_group.create_dataset(\"genparticle_to_calo_hit_matrix/cols\", data=genparticle_to_calo_hit_matrix[1])\n",
    "        event_group.create_dataset(\"genparticle_to_calo_hit_matrix/weights\", data=genparticle_to_calo_hit_matrix[2])\n",
    "\n",
    "        event_group.create_dataset(\"genparticle_to_tracker_hit_matrix/rows\", data=genparticle_to_tracker_hit_matrix[0])\n",
    "        event_group.create_dataset(\"genparticle_to_tracker_hit_matrix/cols\", data=genparticle_to_tracker_hit_matrix[1])\n",
    "        event_group.create_dataset(\n",
    "            \"genparticle_to_tracker_hit_matrix/weights\", data=genparticle_to_tracker_hit_matrix[2]\n",
    "        )\n",
    "\n",
    "        event_group.create_dataset(\"track_to_tracker_hit_matrix/rows\", data=track_to_tracker_hit_matrix[0])\n",
    "        event_group.create_dataset(\"track_to_tracker_hit_matrix/cols\", data=track_to_tracker_hit_matrix[1])\n",
    "        event_group.create_dataset(\"track_to_tracker_hit_matrix/weights\", data=track_to_tracker_hit_matrix[2])\n",
    "\n",
    "        event_group.create_dataset(\"cluster_to_cluster_hit_matrix/rows\", data=cluster_to_cluster_hit_matrix[0])\n",
    "        event_group.create_dataset(\"cluster_to_cluster_hit_matrix/cols\", data=cluster_to_cluster_hit_matrix[1])\n",
    "        event_group.create_dataset(\"cluster_to_cluster_hit_matrix/weights\", data=cluster_to_cluster_hit_matrix[2])\n",
    "\n",
    "        event_group.create_dataset(\"gp_to_track_matrix/rows\", data=gp_to_track_matrix[0])\n",
    "        event_group.create_dataset(\"gp_to_track_matrix/cols\", data=gp_to_track_matrix[1])\n",
    "        event_group.create_dataset(\"gp_to_track_matrix/weights\", data=gp_to_track_matrix[2])\n",
    "\n",
    "        event_group.create_dataset(\"gp_to_gp/rows\", data=gp_to_gp[0])\n",
    "        event_group.create_dataset(\"gp_to_gp/cols\", data=gp_to_gp[1])\n",
    "        event_group.create_dataset(\"gp_to_gp/weights\", data=gp_to_gp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/mnt/ceph/users/ewulff/data/cld/hdf5\")\n",
    "\n",
    "\n",
    "def process_root_files(data_dir, output_dir, events_per_hdf5=100, max_events=None, max_root_files=None):\n",
    "    event_counter = 0\n",
    "    root_counter = 0\n",
    "\n",
    "    output_file = output_dir / Path(f\"events_{event_counter}_to_{event_counter + events_per_hdf5 - 1}.hdf5\")\n",
    "    h5f = h5py.File(output_file, \"w\")\n",
    "    print(f\"Created new HDF5 file: {output_file}\")\n",
    "\n",
    "    for root_file in tqdm(\n",
    "        Path(data_dir).rglob(\"*.root\"),\n",
    "        desc=\"Processing ROOT files\",\n",
    "        total=max_root_files or len(list(Path(data_dir).rglob(\"*.root\"))),\n",
    "    ):\n",
    "        try:\n",
    "            fi = uproot.open(root_file)\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Create a group for the event\n",
    "                event_group = h5f.create_group(f\"event_{event_counter}\")\n",
    "\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Save genparticle features\n",
    "                for key, value in gen_features.items():\n",
    "                    event_group.create_dataset(f\"gen_features/{key}\", data=value)\n",
    "\n",
    "                # Save track features\n",
    "                for key, value in track_features.items():\n",
    "                    event_group.create_dataset(f\"track_features/{key}\", data=value)\n",
    "\n",
    "                # Save cluster features\n",
    "                for key, value in cluster_features.items():\n",
    "                    event_group.create_dataset(f\"cluster_features/{key}\", data=value)\n",
    "\n",
    "                # Save calorimeter hit features\n",
    "                for key, value in calo_hit_features.items():\n",
    "                    event_group.create_dataset(f\"calo_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "                # Save tracker hit features\n",
    "                for key, value in tracker_hit_features.items():\n",
    "                    event_group.create_dataset(f\"tracker_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "                # Save adjacency matrices\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/rows\", data=genparticle_to_calo_hit_matrix[0]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/cols\", data=genparticle_to_calo_hit_matrix[1]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/weights\", data=genparticle_to_calo_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/rows\", data=genparticle_to_tracker_hit_matrix[0]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/cols\", data=genparticle_to_tracker_hit_matrix[1]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/weights\", data=genparticle_to_tracker_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/rows\", data=track_to_tracker_hit_matrix[0])\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/cols\", data=track_to_tracker_hit_matrix[1])\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/weights\", data=track_to_tracker_hit_matrix[2])\n",
    "\n",
    "                event_group.create_dataset(\"cluster_to_cluster_hit_matrix/rows\", data=cluster_to_cluster_hit_matrix[0])\n",
    "                event_group.create_dataset(\"cluster_to_cluster_hit_matrix/cols\", data=cluster_to_cluster_hit_matrix[1])\n",
    "                event_group.create_dataset(\n",
    "                    \"cluster_to_cluster_hit_matrix/weights\", data=cluster_to_cluster_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/rows\", data=gp_to_track_matrix[0])\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/cols\", data=gp_to_track_matrix[1])\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/weights\", data=gp_to_track_matrix[2])\n",
    "\n",
    "                event_group.create_dataset(\"gp_to_gp/rows\", data=gp_to_gp[0])\n",
    "                event_group.create_dataset(\"gp_to_gp/cols\", data=gp_to_gp[1])\n",
    "                event_group.create_dataset(\"gp_to_gp/weights\", data=gp_to_gp[2])\n",
    "\n",
    "                event_counter += 1\n",
    "\n",
    "                if max_events is not None and event_counter >= max_events:\n",
    "                    print(f\"Reached max_events limit: {max_events}. Stopping processing.\")\n",
    "                    h5f.close()\n",
    "                    return\n",
    "\n",
    "                # Check if we need to start a new HDF5 file\n",
    "                if event_counter % events_per_hdf5 == 0:\n",
    "                    h5f.close()\n",
    "                    output_file = output_dir / Path(\n",
    "                        f\"events_{event_counter}_to_{event_counter + events_per_hdf5 - 1}.hdf5\"\n",
    "                    )\n",
    "                    h5f = h5py.File(output_file, \"w\")\n",
    "                    print(f\"Created new HDF5 file: {output_file}\")\n",
    "\n",
    "            print(f\"Processed: {root_file}\")\n",
    "            root_counter += 1\n",
    "\n",
    "            if max_root_files is not None and root_counter >= max_root_files:\n",
    "                print(f\"Reached max_root_files limit: {max_root_files}. Stopping processing.\")\n",
    "                h5f.close()\n",
    "                return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    # Close the last HDF5 file\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_root_files(data_dir=\"/mnt/ceph/users/ewulff/data/cld/\", output_dir=output_dir, events_per_hdf5=50, max_events=None, max_root_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/mnt/ceph/users/ewulff/data/cld/hdf5\")\n",
    "\n",
    "\n",
    "def merge_root_files_to_hdf5(data_dir, output_dir, files_per_hdf5=10):\n",
    "    file_counter = 0\n",
    "    hdf5_counter = 0\n",
    "\n",
    "    output_file = output_dir / Path(f\"merged_features_{hdf5_counter}.hdf5\")\n",
    "    h5f = h5py.File(output_file, \"w\")\n",
    "\n",
    "    for root_file in tqdm(data_dir.rglob(\"*.root\"), desc=\"Processing ROOT files\"):\n",
    "        try:\n",
    "            fi = uproot.open(root_file)\n",
    "            ev = fi[\"events\"]\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Create a group for the event\n",
    "                event_group = h5f.create_group(f\"event_{file_counter}_{iev}\")\n",
    "\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(ev, iev)\n",
    "                track_features = track_to_features(ev, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    ev, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, calo_hit_idx_local_to_global = process_calo_hit_data(\n",
    "                    ev, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, tracker_hit_idx_local_to_global = (\n",
    "                    process_tracker_hit_data(ev, iev, collectionIDs)\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, tracker_hit_idx_local_to_global_2 = create_track_to_hit_coo_matrix(\n",
    "                    ev, iev, collectionIDs\n",
    "                )\n",
    "                cluster_to_cluster_hit_matrix, calo_hit_idx_local_to_global_2 = create_cluster_to_hit_coo_matrix(\n",
    "                    ev, iev, collectionIDs\n",
    "                )\n",
    "                gp_to_track_matrix = genparticle_track_adj(ev, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(ev, iev)\n",
    "\n",
    "                # Save genparticle features\n",
    "                for key, value in gen_features.items():\n",
    "                    event_group.create_dataset(f\"gen_features/{key}\", data=value)\n",
    "\n",
    "                # Save track features\n",
    "                for key, value in track_features.items():\n",
    "                    event_group.create_dataset(f\"track_features/{key}\", data=value)\n",
    "\n",
    "                # Save cluster features\n",
    "                for key, value in cluster_features.items():\n",
    "                    event_group.create_dataset(f\"cluster_features/{key}\", data=value)\n",
    "\n",
    "                # Save calorimeter hit features\n",
    "                for key, value in calo_hit_features.items():\n",
    "                    event_group.create_dataset(f\"calo_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "                # Save tracker hit features\n",
    "                for key, value in tracker_hit_features.items():\n",
    "                    event_group.create_dataset(f\"tracker_hit_features/{key}\", data=awkward.to_numpy(value))\n",
    "\n",
    "                # Save adjacency matrices\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/rows\", data=genparticle_to_calo_hit_matrix[0]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/cols\", data=genparticle_to_calo_hit_matrix[1]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_calo_hit_matrix/weights\", data=genparticle_to_calo_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/rows\", data=genparticle_to_tracker_hit_matrix[0]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/cols\", data=genparticle_to_tracker_hit_matrix[1]\n",
    "                )\n",
    "                event_group.create_dataset(\n",
    "                    \"genparticle_to_tracker_hit_matrix/weights\", data=genparticle_to_tracker_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/rows\", data=track_to_tracker_hit_matrix[0])\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/cols\", data=track_to_tracker_hit_matrix[1])\n",
    "                event_group.create_dataset(\"track_to_tracker_hit_matrix/weights\", data=track_to_tracker_hit_matrix[2])\n",
    "\n",
    "                event_group.create_dataset(\"cluster_to_cluster_hit_matrix/rows\", data=cluster_to_cluster_hit_matrix[0])\n",
    "                event_group.create_dataset(\"cluster_to_cluster_hit_matrix/cols\", data=cluster_to_cluster_hit_matrix[1])\n",
    "                event_group.create_dataset(\n",
    "                    \"cluster_to_cluster_hit_matrix/weights\", data=cluster_to_cluster_hit_matrix[2]\n",
    "                )\n",
    "\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/rows\", data=gp_to_track_matrix[0])\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/cols\", data=gp_to_track_matrix[1])\n",
    "                event_group.create_dataset(\"gp_to_track_matrix/weights\", data=gp_to_track_matrix[2])\n",
    "\n",
    "                event_group.create_dataset(\"gp_to_gp/rows\", data=gp_to_gp[0])\n",
    "                event_group.create_dataset(\"gp_to_gp/cols\", data=gp_to_gp[1])\n",
    "                event_group.create_dataset(\"gp_to_gp/weights\", data=gp_to_gp[2])\n",
    "\n",
    "            file_counter += 1\n",
    "            print(f\"Processed: {root_file}\")\n",
    "\n",
    "            # Check if we need to start a new HDF5 file\n",
    "            if file_counter % files_per_hdf5 == 0:\n",
    "                h5f.close()\n",
    "                hdf5_counter += 1\n",
    "                output_file = output_dir / Path(f\"merged_features_{hdf5_counter}.hdf5\")\n",
    "                h5f = h5py.File(output_file, \"w\")\n",
    "                print(f\"Created new HDF5 file: {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    # Close the last HDF5 file\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back into Python variables\n",
    "def read_event_from_hdf5(file, iev):\n",
    "    with h5py.File(file, \"r\") as h5f:\n",
    "        # Access the event group\n",
    "        event_group = h5f[f\"event_{iev}\"]\n",
    "\n",
    "        # Load genparticle features\n",
    "        gen_features_loaded = {key: event_group[f\"gen_features/{key}\"][:] for key in event_group[\"gen_features\"].keys()}\n",
    "\n",
    "        # Load track features\n",
    "        track_features_loaded = {\n",
    "            key: event_group[f\"track_features/{key}\"][:] for key in event_group[\"track_features\"].keys()\n",
    "        }\n",
    "\n",
    "        # Load cluster features\n",
    "        cluster_features_loaded = {\n",
    "            key: event_group[f\"cluster_features/{key}\"][:] for key in event_group[\"cluster_features\"].keys()\n",
    "        }\n",
    "\n",
    "        # Load calorimeter hit features\n",
    "        calo_hit_features_loaded = {\n",
    "            key: event_group[f\"calo_hit_features/{key}\"][:] for key in event_group[\"calo_hit_features\"].keys()\n",
    "        }\n",
    "\n",
    "        # Load tracker hit features\n",
    "        tracker_hit_features_loaded = {\n",
    "            key: event_group[f\"tracker_hit_features/{key}\"][:] for key in event_group[\"tracker_hit_features\"].keys()\n",
    "        }\n",
    "\n",
    "        # Load adjacency matrices\n",
    "        genparticle_to_calo_hit_matrix_loaded = (\n",
    "            event_group[\"genparticle_to_calo_hit_matrix/rows\"][:],\n",
    "            event_group[\"genparticle_to_calo_hit_matrix/cols\"][:],\n",
    "            event_group[\"genparticle_to_calo_hit_matrix/weights\"][:],\n",
    "        )\n",
    "\n",
    "        genparticle_to_tracker_hit_matrix_loaded = (\n",
    "            event_group[\"genparticle_to_tracker_hit_matrix/rows\"][:],\n",
    "            event_group[\"genparticle_to_tracker_hit_matrix/cols\"][:],\n",
    "            event_group[\"genparticle_to_tracker_hit_matrix/weights\"][:],\n",
    "        )\n",
    "\n",
    "        track_to_tracker_hit_matrix_loaded = (\n",
    "            event_group[\"track_to_tracker_hit_matrix/rows\"][:],\n",
    "            event_group[\"track_to_tracker_hit_matrix/cols\"][:],\n",
    "            event_group[\"track_to_tracker_hit_matrix/weights\"][:],\n",
    "        )\n",
    "\n",
    "        cluster_to_cluster_hit_matrix_loaded = (\n",
    "            event_group[\"cluster_to_cluster_hit_matrix/rows\"][:],\n",
    "            event_group[\"cluster_to_cluster_hit_matrix/cols\"][:],\n",
    "            event_group[\"cluster_to_cluster_hit_matrix/weights\"][:],\n",
    "        )\n",
    "\n",
    "        gp_to_track_matrix_loaded = (\n",
    "            event_group[\"gp_to_track_matrix/rows\"][:],\n",
    "            event_group[\"gp_to_track_matrix/cols\"][:],\n",
    "            event_group[\"gp_to_track_matrix/weights\"][:],\n",
    "        )\n",
    "\n",
    "        gp_to_gp_loaded = (\n",
    "            event_group[\"gp_to_gp/rows\"][:],\n",
    "            event_group[\"gp_to_gp/cols\"][:],\n",
    "            event_group[\"gp_to_gp/weights\"][:],\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        gen_features_loaded,\n",
    "        track_features_loaded,\n",
    "        cluster_features_loaded,\n",
    "        calo_hit_features_loaded,\n",
    "        tracker_hit_features_loaded,\n",
    "        genparticle_to_calo_hit_matrix_loaded,\n",
    "        genparticle_to_tracker_hit_matrix_loaded,\n",
    "        track_to_tracker_hit_matrix_loaded,\n",
    "        cluster_to_cluster_hit_matrix_loaded,\n",
    "        gp_to_track_matrix_loaded,\n",
    "        gp_to_gp_loaded,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = Path(\"extracted_features.hdf5\")\n",
    "# save_event_to_hdf5(\n",
    "#     output_file,\n",
    "#     iev,\n",
    "#     gen_features,\n",
    "#     track_features,\n",
    "#     cluster_features,\n",
    "#     calo_hit_features,\n",
    "#     tracker_hit_features,\n",
    "#     genparticle_to_calo_hit_matrix,\n",
    "#     genparticle_to_tracker_hit_matrix,\n",
    "#     track_to_tracker_hit_matrix,\n",
    "#     cluster_to_cluster_hit_matrix,\n",
    "#     gp_to_track_matrix,\n",
    "#     gp_to_gp\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back from the HDF5 file\n",
    "# (\n",
    "#     gen_features,\n",
    "#     track_features,\n",
    "#     cluster_features,\n",
    "#     calo_hit_features,\n",
    "#     tracker_hit_features,\n",
    "#     genparticle_to_calo_hit_matrix,\n",
    "#     genparticle_to_tracker_hit_matrix,\n",
    "#     track_to_tracker_hit_matrix,\n",
    "#     cluster_to_cluster_hit_matrix,\n",
    "#     gp_to_track_matrix,\n",
    "#     gp_to_gp,\n",
    "# ) = read_event_from_hdf5(output_file, iev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/mnt/ceph/users/ewulff/particlemind/data/zenodo/calohit_challenge\"\n",
    "# file_path = Path(data_dir) / \"dataset_1_photons_1.hdf5\"\n",
    "\n",
    "data_dir = \"/mnt/ceph/users/ewulff/particlemind/notebooks\"\n",
    "file_path = Path(data_dir) / \"extracted_features.hdf5\"\n",
    "\n",
    "\n",
    "def print_h5_contents(file_path):\n",
    "    \"\"\"\n",
    "    Prints the contents of an HDF5 file, including groups and datasets.\n",
    "\n",
    "    Args:\n",
    "        file_path (str or Path): Path to the HDF5 file.\n",
    "    \"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    with h5py.File(file_path, \"r\") as h5f:\n",
    "        # List all groups and datasets in the file\n",
    "        def list_datasets(name, obj):\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                print(f\"Dataset: {name}, Shape: {obj.shape}, Type: {obj.dtype}\")\n",
    "            elif isinstance(obj, h5py.Group):\n",
    "                print(f\"Group: {name}\")\n",
    "\n",
    "        print(\"Contents of the HDF5 file:\")\n",
    "        h5f.visititems(list_datasets)\n",
    "\n",
    "\n",
    "# print_h5_contents(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumps(obj):\n",
    "    \"\"\"\n",
    "    Serialize an object.\n",
    "\n",
    "    Returns:\n",
    "        Implementation-dependent bytes-like object\n",
    "    \"\"\"\n",
    "    return pickle.dumps(obj, protocol=5)\n",
    "\n",
    "\n",
    "def process_root_files_to_lmdb(input_dir, output, max_root_files=None):\n",
    "    \"\"\"\n",
    "    Process ROOT files and save extracted features into an LMDB database.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): Directory containing ROOT files.\n",
    "        output (str or Path): Path to the LMDB database file.\n",
    "        collectionIDs (dict): Mapping of collection names to their IDs.\n",
    "        max_root_files (int, optional): Maximum number of ROOT files to process. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    lmdb_path = os.path.expanduser(output)\n",
    "    isdir = os.path.isdir(lmdb_path)\n",
    "\n",
    "    print(\"Generate LMDB to %s\" % lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=isdir, map_size=1099511627776 * 2, readonly=False, meminit=False, map_async=True)\n",
    "\n",
    "    txn = db.begin(write=True)\n",
    "    event_counter = 0\n",
    "    root_counter = 0\n",
    "\n",
    "    for root_file in tqdm(Path(input_dir).rglob(\"*.root\"), desc=\"Processing ROOT files\"):\n",
    "        if max_root_files is not None and root_counter >= max_root_files:\n",
    "            print(f\"Reached max_root_files limit: {max_root_files}. Stopping processing.\")\n",
    "            break\n",
    "        try:\n",
    "            fi = uproot.open(root_file)\n",
    "            collectionIDs = {\n",
    "                k: v\n",
    "                for k, v in zip(\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\n",
    "                        \"events___idTable/m_collectionIDs\"\n",
    "                    ][0],\n",
    "                )\n",
    "            }\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Save the event data into LMDB\n",
    "                txn.put(\n",
    "                    \"{}\".format(event_counter).encode(\"ascii\"),\n",
    "                    dumps(\n",
    "                        {\n",
    "                            \"gen_features\": gen_features,\n",
    "                            \"track_features\": track_features,\n",
    "                            \"cluster_features\": cluster_features,\n",
    "                            \"calo_hit_features\": calo_hit_features,\n",
    "                            \"tracker_hit_features\": tracker_hit_features,\n",
    "                            \"genparticle_to_calo_hit_matrix\": genparticle_to_calo_hit_matrix,\n",
    "                            \"genparticle_to_tracker_hit_matrix\": genparticle_to_tracker_hit_matrix,\n",
    "                            \"track_to_tracker_hit_matrix\": track_to_tracker_hit_matrix,\n",
    "                            \"cluster_to_cluster_hit_matrix\": cluster_to_cluster_hit_matrix,\n",
    "                            \"gp_to_track_matrix\": gp_to_track_matrix,\n",
    "                            \"gp_to_gp\": gp_to_gp,\n",
    "                        }\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                if event_counter % 100 == 0:\n",
    "                    print(f\"[{event_counter}] events processed\")\n",
    "                    txn.commit()\n",
    "                    txn = db.begin(write=True)\n",
    "\n",
    "                event_counter += 1\n",
    "\n",
    "            root_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    # Finish iterating through all events\n",
    "    txn.commit()\n",
    "    keys = [\"{}\".format(k).encode(\"ascii\") for k in range(event_counter)]\n",
    "    with db.begin(write=True) as txn:\n",
    "        txn.put(b\"__keys__\", dumps(keys))\n",
    "        txn.put(b\"__len__\", dumps(len(keys)))\n",
    "\n",
    "    print(\"Flushing database ...\")\n",
    "    db.sync()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_root_files_to_lmdb(\n",
    "#     input_dir=\"/mnt/ceph/users/ewulff/data/cld/\", output=\"/mnt/ceph/users/ewulff/data/cld/lmdb\", max_root_files=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lmdb database\n",
    "def read_full_lmdb_database(lmdb_path):\n",
    "    lmdb_path = os.path.expanduser(lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=os.path.isdir(lmdb_path), readonly=True, lock=False)\n",
    "\n",
    "    with db.begin() as txn:\n",
    "        keys = pickle.loads(txn.get(b\"__keys__\"))\n",
    "        data = {ii: pickle.loads(txn.get(key)) for ii, key in enumerate(keys)}\n",
    "\n",
    "    db.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "lmdb_data = read_full_lmdb_database(\"/mnt/ceph/users/ewulff/data/cld/lmdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in lmdb_data[0].keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
