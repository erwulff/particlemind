{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58188a6-dea6-42f4-ae03-de74e0b92529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import awkward as ak\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import vector\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import uproot\n",
    "\n",
    "from data_processing.cld_processing import (\n",
    "    get_event_data,\n",
    "    gen_to_features,\n",
    "    track_to_features,\n",
    "    cluster_to_features,\n",
    "    process_calo_hit_data,\n",
    "    process_tracker_hit_data,\n",
    "    create_track_to_hit_coo_matrix,\n",
    "    create_cluster_to_hit_coo_matrix,\n",
    "    genparticle_track_adj,\n",
    "    create_genparticle_to_genparticle_coo_matrix,\n",
    "    create_genparticle_to_genparticle_coo_matrix2,\n",
    "    get_calo_hit_data\n",
    ")\n",
    "\n",
    "#logger = get_pylogger(__name__)\n",
    "\n",
    "vector.register_awkward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ecfb7ba-2387-4e9f-96e9-8a2edd84e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomIterableDataset(IterableDataset):\n",
    "    \"\"\"Custom IterableDataset that loads data from multiple files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        files_list: list,\n",
    "        n_files_at_once: int = None,\n",
    "        max_n_files_per_type: int = None,\n",
    "        shuffle_files: bool = True,\n",
    "        shuffle_data: bool = True,\n",
    "        seed: int = 4697,\n",
    "        seed_shuffle_data: int = 3838,\n",
    "        pad_length: int = 128,\n",
    "        logger_name: str = \"CustomIterableDataset\",\n",
    "        feature_dict: dict = None,\n",
    "        token_reco_cfg: dict = None,\n",
    "        token_id_cfg: dict = None,\n",
    "        load_only_once: bool = False,\n",
    "        shuffle_only_once: bool = False,\n",
    "        random_seed_for_per_file_shuffling: int = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        files_list : list\n",
    "            list with the file names for each type. e.g. a dict like\n",
    "            {\"tbqq\": [\"tbqq_0.root\", ...], \"qcd\": [\"qcd_0.root\", ...], ...}.\n",
    "        n_files_at_once : int, optional\n",
    "            Number of files to load at once. If None, one file per files_dict key\n",
    "            is loaded.\n",
    "        max_n_files_per_type : int, optional\n",
    "            Maximum number of files to use per type. If None, all files are used.\n",
    "            Can be used to use e.g. always the first file from the sorted list of files\n",
    "            in validation.\n",
    "        shuffle_files : bool, optional\n",
    "            Whether to shuffle the list of files.\n",
    "        shuffle_data : bool, optional\n",
    "            Whether to shuffle the data after loading.\n",
    "        seed : int, optional\n",
    "            Random seed.\n",
    "        seed_shuffle_data : int, optional\n",
    "            Random seed for shuffling the data. This is useful if you want to shuffle\n",
    "            the data in the same way for different datasets (e.g. train and val).\n",
    "            The default value is 3838.\n",
    "        pad_length : int, optional\n",
    "            Maximum number of particles per jet. If a jet has more particles, the\n",
    "            first pad_length particles are used, the rest is discarded.\n",
    "        logger_name : str, optional\n",
    "            Name of the logger.\n",
    "        feature_dict : dict, optional\n",
    "            Dictionary with the features to load. The keys are the names of the features\n",
    "            and the values are the preprocessing parameters passed to the\n",
    "            `ak_select_and_preprocess` function.\n",
    "        token_reco_cfg : dict, optional\n",
    "            Dictionary with the configuration to reconstruct the tokenized jetclass files.\n",
    "            If None, this is not used.\n",
    "        token_id_cfg : dict, optional\n",
    "            Dictionary with the tokenization configuration, this is to be used when the\n",
    "            token-id data is to be loaded. If None, this is ignored.\n",
    "        load_only_once : bool, optional\n",
    "            If True, the data is loaded only once and then returned in the same order\n",
    "            in each iteration. NOTE: this is only useful if the whole dataset fits into\n",
    "            memory. If the dataset is too large, this will lead to a memory error.\n",
    "        shuffle_only_once : bool, optional\n",
    "            If True, the data is shuffled only once and then returned in the same order\n",
    "            in each iteration. NOTE: this should only be used for val/test.\n",
    "        random_seed_for_per_file_shuffling : int, optional\n",
    "            Random seed for shuffling the jets within a file. This is useful if you want\n",
    "            to only load a subset of the jets from a file and want to choose different\n",
    "            jets in different training runs.\n",
    "            If load_only_once is False, this is ignored.\n",
    "\n",
    "        **kwargs\n",
    "            Additional keyword arguments.\n",
    "\n",
    "        \"\"\"\n",
    "        if feature_dict is None:\n",
    "            raise ValueError(\"feature_dict must be provided.\")\n",
    "     \n",
    "            \n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.info(f\"Using seed {seed}\")\n",
    "        self.pad_length = pad_length\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.shuffle_files = shuffle_files\n",
    "        self.feature_dict = feature_dict\n",
    "        self.particle_features_list = [feat for feat in self.feature_dict.keys()]\n",
    "        self.seed_shuffle_data = seed_shuffle_data\n",
    "        self.load_only_once = load_only_once\n",
    "        self.shuffle_only_once = shuffle_only_once\n",
    "        self.data_shuffled = False\n",
    "        self.random_seed_for_per_file_shuffling = random_seed_for_per_file_shuffling\n",
    "\n",
    "        if self.random_seed_for_per_file_shuffling is not None:\n",
    "            if not self.load_only_once:\n",
    "                self.logger.warning(\n",
    "                    \"random_seed_for_per_file_shuffling is only used if load_only_once is True.\"\n",
    "                )\n",
    "                self.random_seed_for_per_file_shuffling = None\n",
    "            else:\n",
    "                self.logger.info(\n",
    "                    f\"Using random seed {self.random_seed_for_per_file_shuffling} for per-file shuffling.\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(f\"Using the following particle features: {self.particle_features_list}\")\n",
    "        self.logger.info(f\"pad_length {self.pad_length} for the number of particles per jet.\")\n",
    "        self.logger.info(f\"shuffle_data={self.shuffle_data}\")\n",
    "        self.logger.info(f\"shuffle_files={self.shuffle_files}\")\n",
    "       \n",
    "        self.logger.info(\"Using the following features:\")\n",
    "        for feat, params in self.feature_dict.items():\n",
    "            self.logger.info(f\"- {feat}: {params}\")\n",
    "        self.file_list = []\n",
    "        for file in files_list:\n",
    "            self.file_list.extend(sorted(list(glob.glob(file))))\n",
    "\n",
    "        for file in self.file_list:\n",
    "            self.logger.info(f\" - {file}\")\n",
    "\n",
    "        if self.load_only_once:\n",
    "            logger.warning(\n",
    "                \"load_only_once is True. This means that there will only be the initial data loading.\"\n",
    "            )\n",
    "\n",
    "        # if not specified how many files to use at once, use one file per jet_type\n",
    "        if n_files_at_once is None:\n",
    "            self.n_files_at_once = len(self.files_dict)\n",
    "        else:\n",
    "            if n_files_at_once > len(self.file_list):\n",
    "                self.logger.warning(\n",
    "                    f\"n_files_at_once={n_files_at_once} is larger than the number of files in the\"\n",
    "                    f\" dataset ({len(self.file_list)}).\"\n",
    "                )\n",
    "                self.logger.warning(f\"Setting n_files_at_once to {len(self.file_list)}.\")\n",
    "                self.n_files_at_once = len(self.file_list)\n",
    "            else:\n",
    "                self.n_files_at_once = n_files_at_once\n",
    "\n",
    "        self.logger.info(f\"Will load {self.n_files_at_once} files at a time and combine them.\")\n",
    "\n",
    "        self.file_indices = np.array([0, self.n_files_at_once])\n",
    "        self.file_iterations = len(self.file_list) // self.n_files_at_once\n",
    "        if self.load_only_once:\n",
    "            self.file_iterations = 1\n",
    "\n",
    "        self.current_part_data = None\n",
    "        self.current_part_mask = None\n",
    "        self.token_reco_cfg = token_reco_cfg\n",
    "        self.token_id_cfg = token_id_cfg\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns a generator (i.e. iterator) that goes over the current files list and returns\n",
    "        batches of the corresponding data.\"\"\"\n",
    "        # Iterate over jet_type\n",
    "        self.logger.debug(\"\\n>>> __iter__ called\\n\")\n",
    "        self.file_indices = np.array([0, self.n_files_at_once])\n",
    "\n",
    "        # shuffle the file list\n",
    "        if self.shuffle_files:\n",
    "            self.logger.info(\">>> Shuffling files\")\n",
    "            random.shuffle(self.file_list)\n",
    "            # self.logger.info(\">>> self.file_list:\")\n",
    "            # for filename in self.file_list:\n",
    "            #     self.logger.info(f\" - {filename}\")\n",
    "\n",
    "        # Iterate over files\n",
    "        for j in range(self.file_iterations):\n",
    "            self.logger.debug(20 * \"-\")\n",
    "            # Increment file index if not first iteration\n",
    "            if j > 0:\n",
    "                self.logger.info(\">>> Incrementing file index\")\n",
    "                self.file_indices += self.n_files_at_once\n",
    "\n",
    "            # stop the iteration if self.file_indices[1] is larger than the number of files\n",
    "            # FIXME: this means that the last batch of files (in case the number of files is not\n",
    "            # divisible by self.n_files_at_once) is not used --> fix this\n",
    "            # but if shuffling is used, this should not be a problem\n",
    "            if self.file_indices[1] <= len(self.file_list):\n",
    "                self.load_next_files()\n",
    "\n",
    "                # loop over the current data\n",
    "                for i in range(len(self.current_part_data)):\n",
    "                    yield {\n",
    "                        \"part_features\": self.current_part_data[i],\n",
    "                        \"part_mask\": self.current_part_mask[i],\n",
    "                        \"jet_type_labels_one_hot\": self.current_jet_type_labels_one_hot[i],\n",
    "                        \"jet_type_labels\": torch.argmax(self.current_jet_type_labels_one_hot[i]),\n",
    "                    }\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"returns an iterable which represents an iterator that iterates over the dataset.\"\"\"\n",
    "        return iter(self.get_data())\n",
    "\n",
    "    def load_next_files(self):\n",
    "        if self.load_only_once:\n",
    "            if self.current_part_data is not None:\n",
    "                self.logger.warning(\"Data has already been loaded. Will not load again.\")\n",
    "                self.shuffle_current_data()\n",
    "                return\n",
    "        self.part_data_list = []\n",
    "        self.mask_data_list = []\n",
    "\n",
    "        self.current_files = self.file_list[self.file_indices[0] : self.file_indices[1]]\n",
    "        self.logger.info(f\">>> Loading next files - self.file_indices={self.file_indices}\")\n",
    "        if self.load_only_once:\n",
    "            self.logger.warning(\"Loading data only once. Will not load again.\")\n",
    "            self.logger.warning(\"--> This will be the data for all iterations.\")\n",
    "        for i_file, filename in enumerate(self.current_files):\n",
    "            self.logger.info(f\"{i_file+1} / {len(self.current_files)} : {filename}\")\n",
    "\n",
    "            \"\"\"\n",
    "            if self.token_reco_cfg is not None:\n",
    "                gpu_available = torch.cuda.is_available()\n",
    "                _, ak_x_particles, ak_jet_type_labels = reconstruct_jetclass_file(\n",
    "                    filename_in=filename,\n",
    "                    model_ckpt_path=self.token_reco_cfg[\"ckpt_file\"],\n",
    "                    config_path=self.token_reco_cfg[\"config_file\"],\n",
    "                    start_token_included=self.token_reco_cfg[\"start_token_included\"],\n",
    "                    end_token_included=self.token_reco_cfg[\"end_token_included\"],\n",
    "                    shift_tokens_by_minus_one=self.token_reco_cfg[\"shift_tokens_by_minus_one\"],\n",
    "                    device=\"cuda\" if gpu_available else \"cpu\",\n",
    "                    return_labels=True,\n",
    "                )\n",
    "                self.logger.info(\"Calculating additional kinematic features.\")\n",
    "                ak_x_particles = calc_additional_kinematic_features(ak_x_particles)\n",
    "\n",
    "            elif self.token_id_cfg is not None:\n",
    "                ak_x_particles, ak_jet_type_labels = read_tokenized_jetclass_file(\n",
    "                    filename,\n",
    "                    labels=self.labels_to_load,\n",
    "                    remove_start_token=self.token_id_cfg.get(\"remove_start_token\", False),\n",
    "                    remove_end_token=self.token_id_cfg.get(\"remove_end_token\", False),\n",
    "                    shift_tokens_minus_one=self.token_id_cfg.get(\"shift_tokens_minus_one\", False),\n",
    "                    n_load=self.n_jets_per_file,\n",
    "                    random_seed=self.random_seed_for_per_file_shuffling,\n",
    "                )\n",
    "                ak_x_particles = ak.Array(\n",
    "                    {\n",
    "                        \"part_token_id\": ak_x_particles[\"part_token_id\"],\n",
    "                        \"part_token_id_without_last\": ak_x_particles[\"part_token_id\"][:, :-1],\n",
    "                        \"part_token_id_without_first\": ak_x_particles[\"part_token_id\"][:, 1:],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "            \"\"\"\n",
    "                # read the data from the file\n",
    "            # can add jet features, labels, and p4s here''\n",
    "\n",
    "            fi = uproot.open(filename)\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "\n",
    "            all_event_data = []\n",
    "\n",
    "            collectionIDs = {\n",
    "                    k: v\n",
    "                    for k, v in zip(\n",
    "                        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\"events___idTable/m_collectionIDs\"][0],\n",
    "                    )\n",
    "                    }\n",
    "\n",
    "            for iev in range(2):#len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "            \n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, calo_hit_idx_local_to_global = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                all_event_data.append(calo_hit_features)\n",
    "\n",
    "            all_event_data = ak.Array(all_event_data)\n",
    "\n",
    "            # pad data\n",
    "            padded_data = ak.fill_none(ak.pad_none(all_event_data, self.pad_length, axis=1, clip=True), value=0)\n",
    "            if len(all_event_data.fields) >= 1:\n",
    "                 mask = ak.ones_like(all_event_data[all_event_data.fields[0]], dtype=\"bool\")\n",
    "            else:\n",
    "                mask = ak.ones_like(all_event_data, dtype=\"bool\")\n",
    "            mask = ak.fill_none(ak.pad_none(mask, self.pad_length, axis=1, clip=True), False)\n",
    "\n",
    "\n",
    "            # convert to numpy\n",
    "            np_padded_data = ak.to_numpy(\n",
    "                    np.stack(\n",
    "                        [ak.to_numpy(ak.values_astype(padded_data[name], \"float32\")) for name in self.particle_features_list],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            np_mask = ak.to_numpy(mask)\n",
    "\n",
    "            # add the data to the lists\n",
    "            self.part_data_list.append(torch.tensor(np_padded_data))\n",
    "            self.mask_data_list.append(torch.tensor(np_mask, dtype=torch.bool))\n",
    "        \n",
    "        # concatenate the data from all files\n",
    "        self.current_part_data = torch.cat(self.part_data_list, dim=0)\n",
    "        self.current_part_mask = torch.cat(self.mask_data_list, dim=0)\n",
    "        self.shuffle_current_data()\n",
    "        self.logger.info(\n",
    "            f\">>> Data loaded. (self.current_part_data.shape = {self.current_part_data.shape})\"\n",
    "        )\n",
    "\n",
    "    def shuffle_current_data(self):\n",
    "        # shuffle the data\n",
    "        if self.shuffle_only_once and self.data_shuffled:\n",
    "            self.logger.info(\"Data has already been shuffled. Will not shuffle again.\")\n",
    "            return\n",
    "        if self.shuffle_data:\n",
    "            rng = np.random.default_rng()\n",
    "            if self.seed_shuffle_data is not None:\n",
    "                self.logger.info(f\"Shuffling data with seed {self.seed_shuffle_data}\")\n",
    "                rng = np.random.default_rng(self.seed_shuffle_data)\n",
    "            perm = rng.permutation(len(self.current_part_data))\n",
    "            self.current_part_data = self.current_part_data[perm]\n",
    "            self.current_part_mask = self.current_part_mask[perm]\n",
    "            self.data_shuffled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfc00917-7a94-4cc5-86d1-6bd9ba000145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21010, 31010, 41010, 51010, 61010, ..., 12032, 22032, 32032, 42032, 52032]\n",
      "[0.0266, 0.0917, 0.251, 0.209, ..., 0.00106, 9.51e-05, 7.98e-05, 6.17e-05]\n",
      "[2.16e+03, 2.17e+03, 2.17e+03, 2.18e+03, ..., -2.95e+03, -3.09e+03, -3.24e+03]\n",
      "[35.7, 35.7, 35.7, 35.7, 35.7, ..., -2.33e+03, -2.44e+03, -2.59e+03, -2.7e+03]\n",
      "[1.29e+03, 1.29e+03, 1.29e+03, 1.29e+03, ..., 4.69e+03, 4.93e+03, 5.17e+03]\n",
      "[1010, 11010, 21010, 31010, 41010, ..., 54021, 44021, 64021, 64021, 34021]\n",
      "[0.00764, 0.00662, 0.00763, 0.00654, ..., 0.0155, 0.0521, 0.032, 0.0355]\n",
      "[1.77e+03, 1.78e+03, 1.78e+03, 1.79e+03, 1.79e+03, ..., 182, 235, 240, 283, 132]\n",
      "[-1.23e+03, -1.23e+03, -1.24e+03, ..., -2.58e+03, -2.65e+03, -2.52e+03]\n",
      "[-1.84e+03, -1.85e+03, -1.85e+03, ..., -2.53e+03, -2.53e+03, -2.45e+03]\n",
      "torch.Size([2, 5, 100000]) torch.Size([2, 100000])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomIterableDataset' object has no attribute 'current_jet_type_labels_one_hot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m test_dataloader = DataLoader(test_dataset, batch_size=\u001b[32m3\u001b[39m)\n\u001b[32m     23\u001b[39m iter_dl = \u001b[38;5;28miter\u001b[39m(test_dataloader)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[32m     26\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    n_jets_per_file: int = None,\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    max_n_files_per_type: int = None,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \u001b[33;03m    random_seed_for_per_file_shuffling: int = None,,\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pscratch/sd/r/rmastand/polymathic_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pscratch/sd/r/rmastand/polymathic_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pscratch/sd/r/rmastand/polymathic_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         data.append(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset_iter))\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mCustomIterableDataset.get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# loop over the current data\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.current_part_data)):\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[32m    185\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpart_features\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.current_part_data[i],\n\u001b[32m    186\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpart_mask\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.current_part_mask[i],\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjet_type_labels_one_hot\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_jet_type_labels_one_hot\u001b[49m[i],\n\u001b[32m    188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjet_type_labels\u001b[39m\u001b[33m\"\u001b[39m: torch.argmax(\u001b[38;5;28mself\u001b[39m.current_jet_type_labels_one_hot[i]),\n\u001b[32m    189\u001b[39m     }\n",
      "\u001b[31mAttributeError\u001b[39m: 'CustomIterableDataset' object has no attribute 'current_jet_type_labels_one_hot'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_dir = \"/pscratch/sd/r/rmastand/particlemind/data/p8_ee_tt_ecm365_rootfiles/\"\n",
    "\n",
    "\n",
    "files_list = [f\"{data_dir}/reco_p8_ee_tt_ecm365_63743.root\", f\"{data_dir}/reco_p8_ee_tt_ecm365_63733.root\"]\n",
    "\n",
    "\n",
    "feature_dict = { \"type\": {\"multiply_by\": 0.3, \"subtract_by\": 2.7, \"func\": np.log, \"inv_func\": np.exp},\n",
    "     \"energy\": {\"multiply_by\": 4},\n",
    "     \"position.x\": {\"multiply_by\": 4},\n",
    "               \"position.y\": {\"multiply_by\": 4},\n",
    "               \"position.z\": {\"multiply_by\": 4}}\n",
    "\n",
    "\n",
    "test_dataset = CustomIterableDataset(\n",
    "        files_list = files_list,\n",
    "        n_files_at_once = 1,\n",
    "        feature_dict = feature_dict,\n",
    "    pad_length = 100000,\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=3)\n",
    "\n",
    "iter_dl = iter(test_dataloader)\n",
    "batch = next(iter_dl)\n",
    "print(batch)\n",
    "\"\"\"\n",
    "    n_jets_per_file: int = None,\n",
    "    max_n_files_per_type: int = None,\n",
    "    shuffle_files: bool = True,\n",
    "    shuffle_data: bool = True,\n",
    "    seed: int = 4697,\n",
    "    seed_shuffle_data: int = 3838,\n",
    "    pad_length: int = 128,\n",
    "    logger_name: str = \"CustomIterableDataset\",\n",
    "    feature_dict: dict = None,\n",
    "    labels_to_load: list = None,\n",
    "    token_reco_cfg: dict = None,\n",
    "    token_id_cfg: dict = None,\n",
    "    load_only_once: bool = False,\n",
    "    shuffle_only_once: bool = False,\n",
    "    random_seed_for_per_file_shuffling: int = None,,\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6913dd-8b3c-4e11-827f-a3c63f0de8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6c3ad-e326-4848-b263-1d834f7bcbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001c9ef-6ec4-46f8-b403-72d47962ec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765e85f-dd1a-4df8-a860-aadc84258b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a38372-479f-47e6-a2b6-eda7e0450a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e5375-b77f-4739-b7a8-7644ce5f281d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymatic_env",
   "language": "python",
   "name": "polymatic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
